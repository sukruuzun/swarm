"""
ğŸ–¥ï¸ LOKAL PC TESTÄ°: Drive'dan indirdiÄŸin .pt dosyalarÄ±nÄ± kendi PC'nde Ã§alÄ±ÅŸtÄ±r
===============================================================================
KullanÄ±m:
    1. Drive'dan swarm_model_blocks/ klasÃ¶rÃ¼nÃ¼ indir
    2. Bu script'in yanÄ±na koy:
       
       swarm/
       â”œâ”€â”€ local_pc_test.py          â† Bu dosya
       â”œâ”€â”€ swarm_llm/
       â”‚   â”œâ”€â”€ hf_loader.py
       â”‚   â””â”€â”€ external_router.py
       â””â”€â”€ model_blocks_qwen25_7b/   â† Drive'dan indirdiÄŸin
           â”œâ”€â”€ block_0.pt
           â”œâ”€â”€ block_1.pt
           â”œâ”€â”€ ...
           â”œâ”€â”€ block_6.pt
           â”œâ”€â”€ router.pt
           â””â”€â”€ rotary_emb.pt

    3. Ã‡alÄ±ÅŸtÄ±r:
       python local_pc_test.py

    Gereksinimler:
       pip install torch transformers
"""

import os
import sys

# MPS (Apple Silicon) bellek limitini kaldÄ±r â€” torch'dan ONCE set edilmeli!
# Unified memory olduÄŸu iÃ§in gÃ¼venli â€” sadece 1 blok (~1.8 GB) bellekte
os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"

import torch

# â”€â”€ AYARLAR â”€â”€
SAVE_DIR = "model_blocks_qwen25_7b"  # .pt dosyalarÄ±nÄ±n olduÄŸu klasÃ¶r
MODEL_NAME = "Qwen/Qwen2.5-7B"       # Tokenizer iÃ§in (sadece config indirir, model DEÄÄ°L)
PROMPT = "The history of artificial intelligence is"

print("="*60)
print("ğŸ–¥ï¸  LOKAL PC TESTÄ° â€” Sequential Lazy Loading")
print("="*60)

# Cihaz kontrolÃ¼
if torch.cuda.is_available():
    device = "cuda"
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
    device = "mps"
    print(f"Apple Silicon (MPS) aktif")
else:
    device = "cpu"
    print(f"CPU modu (yavaÅŸ ama Ã§alÄ±ÅŸÄ±r)")

# Dosya kontrolÃ¼
print(f"\nğŸ“‚ Blok dizini: {SAVE_DIR}/")
if not os.path.exists(SAVE_DIR):
    print(f"âŒ '{SAVE_DIR}' klasÃ¶rÃ¼ bulunamadÄ±!")
    print(f"   Drive'dan indirip bu script'in yanÄ±na koy.")
    sys.exit(1)

pt_files = sorted([f for f in os.listdir(SAVE_DIR) if f.endswith('.pt')])
print(f"   Dosyalar: {len(pt_files)} adet")
for f in pt_files:
    size_mb = os.path.getsize(os.path.join(SAVE_DIR, f)) / (1024**2)
    print(f"   ğŸ“„ {f}: {size_mb:.0f} MB")

# Tokenizer yÃ¼kle (sadece config, model DEÄÄ°L â€” Ã§ok kÃ¼Ã§Ã¼k)
print(f"\nğŸ“¥ Tokenizer yÃ¼kleniyor: {MODEL_NAME}...")
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
print(f"   Vocab: {tokenizer.vocab_size}")

# Sequential Lazy Loading
print(f"\nğŸ”„ Sequential Lazy Loading baÅŸlatÄ±lÄ±yor...")
print(f"   Her blok: diskten yÃ¼kle â†’ Ã§alÄ±ÅŸtÄ±r â†’ bellekten sil")
print(f"   VRAM: Sadece 1 blok (~1.8 GB) + router (~2 GB) = ~4 GB")

from swarm_llm.hf_loader import HuggingFaceBlockLoader

loader = HuggingFaceBlockLoader.from_disk_blocks(
    tokenizer=tokenizer,
    save_dir=SAVE_DIR,
    device=device,
    lazy_load=True,
    sequential_all=True,  # TÃœM bloklar sÄ±rayla â†’ tam kalite
)

# Metin Ã¼retimi
print(f"\nğŸ§ª Metin Ã¼retimi baÅŸlÄ±yor...")
print(f"   Prompt: '{PROMPT}'")

generated = loader.generate(
    prompt=PROMPT,
    max_new_tokens=10,  # Her token ~1 dk disk I/O, 10 token = ~10 dk
    temperature=0.8,
    top_k=40,
)

print(f"\n{'='*60}")
print(f"ğŸ“ ÃœRETÄ°LEN METÄ°N:")
print(f"{'='*60}")
print(f"'{generated}'")
print(f"{'='*60}")

# Bellek durumu
if torch.cuda.is_available():
    allocated = torch.cuda.memory_allocated() / 1e9
    print(f"\nğŸ’¾ GPU VRAM: {allocated:.1f} GB kullanÄ±ldÄ± (tam model ~14 GB olurdu)")

loader.stop_prefetching()

print(f"\nâœ… TEST TAMAMLANDI!")
print(f"   Qwen 7B modeli {device.upper()}'de sequential lazy loading ile Ã§alÄ±ÅŸtÄ±.")
print(f"   Tam kalite, dÃ¼ÅŸÃ¼k bellek kullanÄ±mÄ±.")
