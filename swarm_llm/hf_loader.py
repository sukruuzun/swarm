"""
HuggingFace Model Loader (HF Entegrasyonu)
==========================================
Llama, Qwen gibi eÄŸitilmiÅŸ dev modelleri bloklara bÃ¶lÃ¼p,
Parisi-Nash router ile dinamik yÃ¼kleme yapan wrapper.

KullanÄ±m:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from swarm_llm.hf_loader import HuggingFaceBlockLoader

    tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
    model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")

    loader = HuggingFaceBlockLoader(
        model=model,
        tokenizer=tokenizer,
        num_blocks=8,  # 32 layer â†’ 8 blok x 4 layer
        top_k=2,        # Her forward'da sadece 2 blok
    )

    # Tahmin: hangi bloklar gerekli?
    prompt = "The history of science"
    block_indices, weights = loader.predict_blocks(prompt)
    print(f"YÃ¼klenecek bloklar: {block_indices}")

    # Forward: sadece seÃ§ilen bloklar Ã§alÄ±ÅŸÄ±r
    outputs = loader.generate(prompt, max_new_tokens=50)
"""

import torch
import torch.nn as nn
from typing import List, Optional, Tuple
import threading
from queue import Queue

from swarm_llm.external_router import ExternalParisiNashRouter


class QwenBlockWrapper(nn.Module):
    """
    Qwen2 iÃ§in Ã¶zel block wrapper.
    Sequential iÃ§indeki katmanlara position_embeddings geÃ§irmek iÃ§in.
    """
    def __init__(self, layers: list):
        super().__init__()
        self.layers = nn.ModuleList(layers)
    
    def forward(self, x, position_embeddings=None, position_ids=None, attention_mask=None):
        """
        Qwen2 katmanlarÄ±nÄ± sÄ±rayla Ã§alÄ±ÅŸtÄ±r, position_embeddings'i geÃ§ir.
        """
        for layer in self.layers:
            if position_embeddings is not None:
                x = layer(x, position_embeddings=position_embeddings, 
                         position_ids=position_ids, 
                         attention_mask=attention_mask)
            else:
                x = layer(x)
        return x


class TupleCleaner(nn.Module):
    """
    HuggingFace layer Ã§Ä±ktÄ±larÄ±nÄ± temizleyen wrapper.
    GPT-2/Llama/Qwen layer'larÄ± tuple dÃ¶ndÃ¼rÃ¼r (hidden_states, past_key_values, ...),
    bu wrapper sadece hidden_states'i alÄ±r ve bir sonraki layer'a geÃ§irir.
    
    KRÄ°TÄ°K: Qwen2 gibi modern modeller position_embeddings bekler (RoPE iÃ§in).
    Bu parametreler katmanlara geÃ§irilmelidir.
    """
    def __init__(self, layer: nn.Module):
        super().__init__()
        self.layer = layer
    
    def forward(self, x, position_embeddings=None, attention_mask=None, position_ids=None, **kwargs):
        """
        HuggingFace layer Ã§Ä±ktÄ±sÄ±nÄ± temizle.
        
        KRÄ°TÄ°K NOT: 
        - Bu wrapper past_key_values (KV Cache) verisini kaybediyor.
        - Qwen2 gibi modeller position_embeddings bekler (RoPE iÃ§in).
        - Bu parametreler katmana geÃ§irilir ama Ã§Ä±ktÄ±da sadece hidden_states dÃ¶ndÃ¼rÃ¼lÃ¼r.
        
        Args:
            x: hidden_states (Tensor)
            position_embeddings: RoPE iÃ§in position embeddings (Qwen2 iÃ§in gerekli)
            attention_mask: Attention mask
            position_ids: Position IDs
            **kwargs: DiÄŸer parametreler (use_cache, vb.)
        """
        # Qwen2 katmanlarÄ± position_embeddings bekler
        # EÄŸer verilmiÅŸse geÃ§ir, yoksa sadece x'i geÃ§ir (eski modeller iÃ§in)
        if position_embeddings is not None:
            out = self.layer(
                x,
                position_embeddings=position_embeddings,
                attention_mask=attention_mask,
                position_ids=position_ids,
                use_cache=False,  # KV Cache kullanmÄ±yoruz
                **kwargs
            )
        else:
            # Eski modeller iÃ§in (GPT-2, Llama-1, vb.)
            out = self.layer(x, **kwargs)
        
        # Tuple ise sadece hidden_states'i al (past_key_values kaybolur)
        if isinstance(out, tuple):
            return out[0]
        return out


class HuggingFaceBlockLoader(nn.Module):
    """
    HuggingFace modelini bloklara bÃ¶len ve Parisi-Nash router ile
    dinamik yÃ¼kleme yapan wrapper. SÄ±fÄ±r eÄŸitim maliyeti: mevcut
    modelin aÄŸÄ±rlÄ±klarÄ±nÄ± deÄŸiÅŸtirmez, sadece hangi bloklarÄ±n
    Ã§alÄ±ÅŸtÄ±rÄ±lacaÄŸÄ±na karar verir.
    """

    def __init__(
        self,
        model: nn.Module,
        tokenizer,
        num_blocks: int = 8,
        top_k: int = 2,
        layers_per_block: Optional[int] = None,
        embed_dim: Optional[int] = None,
        device: str = "auto",
        sticky_duration: int = 25,  # Sticky routing: kaÃ§ token boyunca bloklar sabit kalÄ±r
        no_sharding: bool = False,  # Test modu: Modeli hiÃ§ bÃ¶lmeden tek blok olarak Ã§alÄ±ÅŸtÄ±r
    ):
        """
        Args:
            model: HuggingFace AutoModelForCausalLM (Llama, Qwen, vb.)
            tokenizer: HuggingFace tokenizer
            num_blocks: Modeli kaÃ§ bloÄŸa bÃ¶leceÄŸiz (Ã¶rn. 8)
            top_k: Her forward'da kaÃ§ blok Ã§alÄ±ÅŸacak (Ã¶rn. 2)
            layers_per_block: Her blokta kaÃ§ layer (None ise otomatik hesapla)
            embed_dim: Embedding boyutu (None ise model'den al)
            device: 'auto', 'cuda', 'cpu'
        """
        super().__init__()
        self.model = model
        self.tokenizer = tokenizer
        self.num_blocks = num_blocks
        self.top_k = top_k

        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)

        # Model'i cihaza taÅŸÄ± (accelerate ile daÄŸÄ±tÄ±lmÄ±ÅŸ modeller iÃ§in kontrol)
        # device_map="auto" kullanÄ±ldÄ±ÄŸÄ±nda model zaten GPU/CPU'ya daÄŸÄ±tÄ±lmÄ±ÅŸtÄ±r
        # ve tekrar taÅŸÄ±nmaya Ã§alÄ±ÅŸÄ±lamaz
        if hasattr(self.model, "hf_device_map") or hasattr(self.model, "device_map"):
            # Model zaten accelerate ile daÄŸÄ±tÄ±lmÄ±ÅŸ, taÅŸÄ±ma
            # Ana cihazÄ± ilk parametrenin device'Ä±ndan al
            try:
                first_param = next(self.model.parameters())
                self.device = first_param.device
                print(f"â„¹ï¸  Model zaten {self.device} Ã¼zerinde daÄŸÄ±tÄ±lmÄ±ÅŸ durumda (accelerate).")
            except:
                # Parametre bulunamazsa varsayÄ±lan cihazÄ± kullan
                pass
        else:
            # Model henÃ¼z daÄŸÄ±tÄ±lmamÄ±ÅŸ (GPT-2 gibi kÃ¼Ã§Ã¼k modeller), taÅŸÄ±
            self.model.to(self.device)

        # KRÄ°TÄ°K: Ã–nce layers'Ä± Ã§Ä±kar (rotary_emb layers'a baÄŸlÄ±)
        # Sonra Qwen kontrolÃ¼ ve rotary embeddings
        self._is_qwen = self._detect_qwen_model()
        
        # Layers'Ä± Ã–NCE Ã§Ä±kar (_extract_rotary_embeddings self.layers'a eriÅŸiyor)
        self.layers = self._extract_layers()
        
        self._rotary_emb = None
        if self._is_qwen and self.model is not None:
            self._rotary_emb = self._extract_rotary_embeddings()

        # NO-SHARDING TEST MODU: Modeli hiÃ§ bÃ¶lmeden tek blok olarak Ã§alÄ±ÅŸtÄ±r
        # Bu mod, sorunun sharding'den mi yoksa model yÃ¼kleme/tokenizer'dan mÄ± kaynaklandÄ±ÄŸÄ±nÄ± test eder
        self.no_sharding = no_sharding
        
        if no_sharding:
            # TÃ¼m katmanlarÄ± tek bir blok olarak kullan
            print("âš ï¸  NO-SHARDING TEST MODU: Model hiÃ§ bÃ¶lÃ¼nmeden tek blok olarak Ã§alÄ±ÅŸacak")
            total_layers = len(self.layers)
            self.num_blocks = 1
            self.top_k = 1
            self.layers_per_block = total_layers
            # TÃ¼m katmanlarÄ± tek bir blokta topla
            wrapped_layers = [TupleCleaner(layer) for layer in self.layers]
            # Qwen iÃ§in Ã¶zel wrapper kullan
            if self._is_qwen:
                self.blocks = nn.ModuleList([QwenBlockWrapper(wrapped_layers)])
            else:
                self.blocks = nn.ModuleList([nn.Sequential(*wrapped_layers)])
            print(f"   TÃ¼m {total_layers} katman tek blokta: Block 0")
        else:
            # Normal mod: Modeli bloklara bÃ¶l
            total_layers = len(self.layers)

            if layers_per_block is None:
                layers_per_block = max(1, total_layers // num_blocks)
            self.layers_per_block = layers_per_block

            # BloklarÄ± oluÅŸtur
            self.blocks = self._create_blocks()

        # Embedding boyutunu bul
        if embed_dim is None:
            embed_dim = self._get_embed_dim()
        self.embed_dim = embed_dim

        # Router oluÅŸtur (no_sharding modunda router gerekli deÄŸil ama yine de oluÅŸtur)
        if no_sharding:
            # No-sharding modunda router kullanÄ±lmayacak ama yine de oluÅŸtur (API uyumluluÄŸu iÃ§in)
            self.router = ExternalParisiNashRouter(
                embed_dim=embed_dim,
                num_blocks=1,
                top_k=1,
            ).to(self.device)
        else:
            self.router = ExternalParisiNashRouter(
                embed_dim=embed_dim,
                num_blocks=num_blocks,
                top_k=top_k,
            ).to(self.device)
        
        # KRÄ°TÄ°K: Router'Ä± modelin dtype'Ä±na cast et
        # Model float16 iken router float32 â†’ LayerNorm'da dtype mismatch
        if self.model is not None:
            try:
                model_dtype = next(self.model.parameters()).dtype
                if model_dtype != torch.float32:
                    self.router = self.router.to(dtype=model_dtype)
                    print(f"   Router dtype: {model_dtype} (model ile eÅŸleÅŸtirildi)")
            except StopIteration:
                pass

        # Embedding layer'Ä± bul
        self.embed_layer = self._get_embed_layer()
        
        # KRÄ°TÄ°K: Tokenizer ve Model vocab_size kontrolÃ¼
        # Bu kontrol, karakter kaymasÄ± (offset) hatalarÄ±nÄ± Ã¶nler
        self._validate_vocab_alignment()
        
        # KRÄ°TÄ°K: Embedding layer'a padding ekle (vocab size mismatch iÃ§in)
        self._pad_embedding_layer()
        
        # Lazy loading iÃ§in
        self._lazy_load = False
        self._block_paths = []
        self._loaded_blocks = {}
        
        # KorumalÄ± bloklar (thrashing'i Ã¶nlemek iÃ§in)
        # Block 0 ve Block 1 varsayÄ±lan olarak kilitli (en sÄ±k kullanÄ±lan bloklar)
        # Block 0: Temel dil yapÄ±sÄ± (alfabe, baÄŸlaÃ§lar)
        # Block 1: Ä°lk transformer katmanlarÄ± (sÄ±k kullanÄ±lÄ±r)
        self._locked_blocks = {0, 1}  # Set: {0, 1, ...} manuel olarak kilitlenebilir
        
        # Blok kullanÄ±m takibi (cleanup iÃ§in)
        self._block_usage_count = {}  # {block_idx: kullanÄ±m_sayÄ±sÄ±}
        
        # Sticky Routing: Bir kez seÃ§ilen bloklar belirli bir sÃ¼re sabit kalÄ±r
        # Bu sayede her token iÃ§in router Ã§alÄ±ÅŸtÄ±rmak yerine, bloklar sabitlenir
        # Ã–rnek: "Tarih" konusu iÃ§in Blok 2 ve 3 seÃ§ildiyse, sonraki 25 token boyunca
        # aynÄ± bloklar kullanÄ±lÄ±r (SSD trafiÄŸi biter, metin akÄ±cÄ±laÅŸÄ±r)
        self._sticky_blocks = None  # Åu an sabitlenmiÅŸ bloklar (set veya None)
        self._sticky_until_token = 0  # Hangi token'a kadar sabit kalacak
        self._sticky_duration = sticky_duration  # KaÃ§ token boyunca sabit kalÄ±r

    def _extract_layers(self) -> nn.ModuleList:
        """
        Model'den transformer layer'larÄ±nÄ± Ã§Ä±kar.
        Desteklenen yapÄ±lar:
        - Llama: model.model.layers
        - Qwen: model.model.layers veya model.layers
        - GPT-2: model.transformer.h
        - Mistral: model.model.layers
        """
        # Qwen/Llama/Mistral: model.model.layers
        if hasattr(self.model, "model") and hasattr(self.model.model, "layers"):
            return self.model.model.layers
        # BazÄ± Qwen varyantlarÄ±: model.layers
        elif hasattr(self.model, "layers"):
            return self.model.layers
        # GPT-2: model.transformer.h
        elif hasattr(self.model, "transformer") and hasattr(self.model.transformer, "h"):
            return self.model.transformer.h
        else:
            # Debug: Model yapÄ±sÄ±nÄ± gÃ¶ster
            print(f"âš ï¸  Model yapÄ±sÄ±: {type(self.model)}")
            if hasattr(self.model, "model"):
                print(f"   model.model: {type(self.model.model)}")
                if hasattr(self.model.model, "__dict__"):
                    print(f"   model.model attributes: {list(self.model.model.__dict__.keys())[:10]}")
            raise ValueError(
                "Model yapÄ±sÄ± desteklenmiyor. Llama/Qwen/Mistral gibi modeller bekleniyor.\n"
                f"Model tipi: {type(self.model)}\n"
                "LÃ¼tfen model yapÄ±sÄ±nÄ± kontrol edin veya issue aÃ§Ä±n."
            )

    def _detect_qwen_model(self) -> bool:
        """Qwen modeli olup olmadÄ±ÄŸÄ±nÄ± tespit et."""
        if self.model is None:
            return False
        model_type = type(self.model).__name__.lower()
        return 'qwen' in model_type or 'qwen2' in model_type
    
    def _extract_rotary_embeddings(self):
        """
        Qwen modelinden rotary embeddings'i Ã§Ä±kar.
        Yeni transformers: model.model.rotary_emb (model seviyesinde)
        Eski transformers: layers[0].self_attn.rotary_emb (layer seviyesinde)
        """
        if self.model is None:
            return None
        
        rotary_emb = None
        
        # YÃ¶ntem 1: model.model.rotary_emb (yeni transformers >= 4.38)
        try:
            if hasattr(self.model, 'model') and hasattr(self.model.model, 'rotary_emb'):
                rotary_emb = self.model.model.rotary_emb
                print(f"âœ… Rotary emb bulundu: model.model.rotary_emb ({type(rotary_emb).__name__})")
                return rotary_emb
        except Exception:
            pass
        
        # YÃ¶ntem 2: layers[0].self_attn.rotary_emb (eski transformers)
        try:
            if self.layers is not None and len(self.layers) > 0:
                first_layer = self.layers[0]
                if hasattr(first_layer, 'self_attn') and hasattr(first_layer.self_attn, 'rotary_emb'):
                    rotary_emb = first_layer.self_attn.rotary_emb
                    print(f"âœ… Rotary emb bulundu: layers[0].self_attn.rotary_emb ({type(rotary_emb).__name__})")
                    return rotary_emb
        except Exception:
            pass
        
        print(f"âš ï¸  Rotary embeddings bulunamadÄ±. Model yapÄ±sÄ±nÄ± kontrol edin.")
        if hasattr(self.model, 'model'):
            attrs = [a for a in dir(self.model.model) if 'rotary' in a.lower() or 'rope' in a.lower()]
            print(f"   model.model'deki rotary/rope attributes: {attrs}")
        if self.layers is not None and len(self.layers) > 0:
            layer0 = self.layers[0]
            if hasattr(layer0, 'self_attn'):
                attrs = [a for a in dir(layer0.self_attn) if 'rotary' in a.lower() or 'rope' in a.lower()]
                print(f"   layers[0].self_attn'deki rotary/rope attributes: {attrs}")
        return None
    
    def _get_embed_layer(self) -> nn.Module:
        """
        Embedding layer'Ä± bul.
        Desteklenen yapÄ±lar:
        - Llama/Qwen: model.model.embed_tokens
        - GPT-2: model.transformer.wte
        - BazÄ± modeller: model.embed_tokens
        """
        # Qwen/Llama: model.model.embed_tokens
        if hasattr(self.model, "model") and hasattr(self.model.model, "embed_tokens"):
            return self.model.model.embed_tokens
        # GPT-2: model.transformer.wte
        elif hasattr(self.model, "transformer") and hasattr(self.model.transformer, "wte"):
            return self.model.transformer.wte
        # BazÄ± modeller: model.embed_tokens
        elif hasattr(self.model, "embed_tokens"):
            return self.model.embed_tokens
        else:
            # Debug bilgisi
            print(f"âš ï¸  Embedding layer bulunamadÄ±. Model yapÄ±sÄ±:")
            if hasattr(self.model, "model"):
                print(f"   model.model attributes: {[k for k in dir(self.model.model) if 'embed' in k.lower()]}")
            raise ValueError(
                "Embedding layer bulunamadÄ±.\n"
                f"Model tipi: {type(self.model)}\n"
                "LÃ¼tfen model yapÄ±sÄ±nÄ± kontrol edin."
            )

    def _validate_vocab_alignment(self):
        """
        Tokenizer ve Model vocab_size'larÄ±nÄ±n eÅŸleÅŸtiÄŸini kontrol et.
        Bu kontrol, karakter kaymasÄ± (offset) hatalarÄ±nÄ± Ã¶nler.
        Ã–rnek: Model 'A' demek isterken 'ç„˜' (Ã‡ince karakter) basmasÄ±.
        
        KRÄ°TÄ°K: Model sayÄ±larla konuÅŸur. Her kelimenin bir ID'si vardÄ±r.
        - Tokenizer: "Tarih" kelimesine 150 diyor
        - Model: 150 sayÄ±sÄ±nÄ± iÅŸliyor ve "200" diyor
        - LM Head: EÄŸer offset varsa, 200 "bilgi" yerine "ç„˜" olabilir
        """
        # Tokenizer vocab_size
        tokenizer_vocab_size = None
        if hasattr(self.tokenizer, 'vocab_size'):
            tokenizer_vocab_size = self.tokenizer.vocab_size
        elif hasattr(self.tokenizer, 'get_vocab'):
            tokenizer_vocab_size = len(self.tokenizer.get_vocab())
        
        # Embedding vocab_size
        embed_vocab_size = None
        if hasattr(self.embed_layer, 'weight'):
            embed_vocab_size = self.embed_layer.weight.shape[0]
        
        # LM Head vocab_size
        lm_head_vocab_size = None
        if self.model is not None:
            if hasattr(self.model, 'lm_head') and hasattr(self.model.lm_head, 'weight'):
                lm_head_vocab_size = self.model.lm_head.weight.shape[0]
        
        # Kontrol ve uyarÄ±
        vocab_sizes = {
            'tokenizer': tokenizer_vocab_size,
            'embedding': embed_vocab_size,
            'lm_head': lm_head_vocab_size,
        }
        
        # TÃ¼m vocab_size'larÄ± yazdÄ±r (debug iÃ§in)
        print(f"ğŸ“Š Vocab Size KontrolÃ¼ (SayÄ±sal KarÅŸÄ±lÄ±k KontrolÃ¼):")
        for name, size in vocab_sizes.items():
            if size is not None:
                print(f"   {name}: {size}")
            else:
                print(f"   {name}: BulunamadÄ±")
        
        # EÅŸleÅŸme kontrolÃ¼
        sizes = [v for v in vocab_sizes.values() if v is not None]
        if len(sizes) > 1:
            if len(set(sizes)) > 1:
                print(f"\nâš ï¸  KRÄ°TÄ°K UYARI: Vocab size uyumsuzluÄŸu tespit edildi!")
                print(f"   Bu durum karakter kaymasÄ± (offset) hatalarÄ±na neden olur.")
                print(f"   Ã–rnek: Model 'A' demek isterken 'ç„˜' (Ã‡ince karakter) basabilir.")
                print(f"\n   ğŸ” Analiz:")
                print(f"   - Tokenizer vocab_size: {tokenizer_vocab_size}")
                print(f"   - Embedding vocab_size: {embed_vocab_size}")
                print(f"   - LM Head vocab_size: {lm_head_vocab_size}")
                print(f"\n   ğŸ’¡ Ã‡Ã¶zÃ¼m:")
                if embed_vocab_size and lm_head_vocab_size:
                    if embed_vocab_size == lm_head_vocab_size:
                        print(f"   - Embedding ve LM Head eÅŸleÅŸiyor ({embed_vocab_size})")
                        print(f"   - Sorun: Tokenizer ile Model arasÄ±nda offset var")
                        offset = embed_vocab_size - (tokenizer_vocab_size or 0)
                        print(f"   - Offset: {offset} token (Model daha bÃ¼yÃ¼k)")
                        print(f"   - Model checkpoint'teki vocab_size kullanÄ±lacak: {embed_vocab_size}")
                        
                        # KRÄ°TÄ°K: Tokenizer vocab_size'Ä± model vocab_size'Ä±na eÅŸitle
                        # Padding token'larÄ± ekle veya tokenizer'Ä± gÃ¼ncelle
                        if tokenizer_vocab_size and offset > 0:
                            print(f"\n   ğŸ”§ Tokenizer Padding: {offset} dummy token ekleniyor...")
                            self._pad_tokenizer_vocab(tokenizer_vocab_size, embed_vocab_size)
                    else:
                        print(f"   - Embedding ({embed_vocab_size}) != LM Head ({lm_head_vocab_size})")
                        print(f"   - Bu durum ciddi bir sorun! Checkpoint'i kontrol edin.")
                print(f"\n   ğŸ§ª Test: no_sharding=True ile test edin")
                print(f"   - Ã‡alÄ±ÅŸÄ±yorsa: Sorun sharding'de")
                print(f"   - Hala bozuksa: Sorun vocab mapping'de")
            else:
                print(f"âœ… Vocab size'lar eÅŸleÅŸiyor: {sizes[0]}")
        
        # Token ID mapping kontrolÃ¼ (opsiyonel ama Ã¶nerilir)
        # Test: Basit bir token'Ä±n ID'sini kontrol et
        try:
            test_tokens = ["The", "history", "of"]
            print(f"\nğŸ” Token ID Mapping KontrolÃ¼:")
            for test_token in test_tokens:
                try:
                    token_id = self.tokenizer.encode(test_token, add_special_tokens=False)[0]
                    decoded = self.tokenizer.decode([token_id])
                    print(f"   '{test_token}' â†’ ID: {token_id} â†’ Decode: '{decoded}'")
                    
                    # ID'nin vocab_size iÃ§inde olduÄŸundan emin ol
                    if embed_vocab_size and token_id >= embed_vocab_size:
                        print(f"   âš ï¸  UYARI: Token ID {token_id} >= Embedding vocab_size {embed_vocab_size}")
                        print(f"      Bu durum IndexError'a neden olabilir!")
                except Exception as e:
                    print(f"   âš ï¸  Token '{test_token}' kontrol edilemedi: {e}")
        except Exception as e:
            print(f"   Token ID mapping kontrolÃ¼ atlandÄ±: {e}")
    
    def _pad_tokenizer_vocab(self, current_size: int, target_size: int):
        """
        Tokenizer vocab_size'Ä± model vocab_size'Ä±na eÅŸitlemek iÃ§in padding token'larÄ± ekle.
        
        KRÄ°TÄ°K: Bu fonksiyon tokenizer'Ä±n vocab_size'Ä±nÄ± artÄ±rmaz ama
        embedding layer'Ä±n beklediÄŸi token ID'lerinin geÃ§erli olduÄŸundan emin olur.
        
        Not: HuggingFace tokenizer'larÄ±n vocab_size'Ä±nÄ± deÄŸiÅŸtirmek zor olduÄŸu iÃ§in,
        bu fonksiyon sadece uyarÄ± verir ve embedding layer'Ä±n padding'i handle etmesini bekler.
        """
        offset = target_size - current_size
        if offset > 0:
            print(f"   âš ï¸  Tokenizer vocab_size ({current_size}) < Model vocab_size ({target_size})")
            print(f"   âš ï¸  Offset: {offset} token")
            print(f"   ğŸ’¡ Not: Tokenizer'Ä±n vocab_size'Ä±nÄ± deÄŸiÅŸtirmek zor.")
            print(f"   ğŸ’¡ Ã‡Ã¶zÃ¼m: Embedding layer padding'i handle edecek.")
            print(f"   ğŸ’¡ Tokenizer token ID'leri 0-{current_size-1} arasÄ±, Model 0-{target_size-1} bekliyor.")
            print(f"   ğŸ’¡ EÄŸer tokenizer token ID >= {current_size} kullanÄ±rsa IndexError oluÅŸabilir.")
    
    def _pad_embedding_layer(self):
        """
        Embedding layer'a padding ekle (vocab size mismatch iÃ§in).
        
        KRÄ°TÄ°K: EÄŸer tokenizer vocab_size < embedding vocab_size ise,
        embedding layer'Ä±n son token'larÄ± kullanÄ±lmÄ±yor olabilir.
        Bu durumda embedding layer'Ä± tokenizer vocab_size'a gÃ¶re clamp edebiliriz
        veya padding token'larÄ± ekleyebiliriz.
        
        Ancak ÅŸu an iÃ§in sadece kontrol yapÄ±yoruz, gerÃ§ek padding eklemiyoruz
        Ã§Ã¼nkÃ¼ model'in aÄŸÄ±rlÄ±klarÄ±nÄ± deÄŸiÅŸtirmek istemiyoruz.
        """
        if self.model is None:
            return
        
        # Tokenizer vocab_size
        tokenizer_vocab_size = None
        if hasattr(self.tokenizer, 'vocab_size'):
            tokenizer_vocab_size = self.tokenizer.vocab_size
        elif hasattr(self.tokenizer, 'get_vocab'):
            tokenizer_vocab_size = len(self.tokenizer.get_vocab())
        
        # Embedding vocab_size
        embed_vocab_size = None
        if hasattr(self.embed_layer, 'weight'):
            embed_vocab_size = self.embed_layer.weight.shape[0]
        
        # EÄŸer tokenizer vocab_size < embedding vocab_size ise
        if tokenizer_vocab_size and embed_vocab_size and tokenizer_vocab_size < embed_vocab_size:
            offset = embed_vocab_size - tokenizer_vocab_size
            print(f"\nğŸ’¡ Embedding Padding Bilgisi:")
            print(f"   - Tokenizer vocab_size: {tokenizer_vocab_size}")
            print(f"   - Embedding vocab_size: {embed_vocab_size}")
            print(f"   - Offset: {offset} token (embedding'in son {offset} token'Ä± kullanÄ±lmÄ±yor)")
            print(f"   - Tokenizer token ID'leri 0-{tokenizer_vocab_size-1} arasÄ±")
            print(f"   - Model embedding 0-{embed_vocab_size-1} arasÄ± bekliyor")
            print(f"   - Bu durum normal, model'in son token'larÄ± padding iÃ§in olabilir")
    
    def _get_embed_dim(self) -> int:
        """Embedding boyutunu bul."""
        embed_layer = self._get_embed_layer()
        if hasattr(embed_layer, "embedding_dim"):
            return embed_layer.embedding_dim
        elif hasattr(embed_layer, "weight"):
            return embed_layer.weight.shape[1]
        else:
            return 4096  # VarsayÄ±lan (Llama-2-7b)

    def _create_blocks(self) -> nn.ModuleList:
        """
        Layer'larÄ± bloklara bÃ¶l.
        Her layer'Ä± TupleCleaner ile sararak tuple Ã§Ä±ktÄ±larÄ±nÄ± temizleriz.
        
        KRÄ°TÄ°K: Qwen2 gibi modeller position_embeddings bekler.
        Qwen iÃ§in Ã¶zel QwenBlockWrapper kullanÄ±lÄ±r.
        """
        blocks = nn.ModuleList()
        total_layers = len(self.layers)

        for i in range(self.num_blocks):
            start_idx = i * self.layers_per_block
            end_idx = min((i + 1) * self.layers_per_block, total_layers)
            if start_idx < total_layers:
                block_layers = self.layers[start_idx:end_idx]
                # Her layer'Ä± TupleCleaner ile sar (tuple Ã§Ä±ktÄ±larÄ±nÄ± temizle)
                wrapped_layers = [TupleCleaner(layer) for layer in block_layers]
                
                # Qwen iÃ§in Ã¶zel wrapper kullan (position_embeddings geÃ§irmek iÃ§in)
                if self._is_qwen:
                    blocks.append(QwenBlockWrapper(wrapped_layers))
                else:
                    blocks.append(nn.Sequential(*wrapped_layers))
            else:
                # BoÅŸ blok (padding)
                blocks.append(nn.Identity())

        return blocks

    @torch.no_grad()
    def predict_blocks(self, prompt: str, prefetch: bool = True) -> Tuple[List[int], torch.Tensor]:
        """
        Teoreminin beyni: GiriÅŸ cÃ¼mlesine bakarak hangi bloklarÄ±n
        gerekli olduÄŸunu tahmin eder. Modeli Ã§alÄ±ÅŸtÄ±rmadan Ã¶nce Ã§aÄŸrÄ±lÄ±r.

        Args:
            prompt: GiriÅŸ metni
            prefetch: True ise tahmin edilen bloklarÄ± arka planda yÃ¼klemeye baÅŸlar

        Returns:
            block_indices: [i1, i2, ...] yÃ¼klenecek blok indeksleri
            weights: (top_k,) blok aÄŸÄ±rlÄ±klarÄ±
        """
        self.eval()
        input_ids = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)
        x = self.embed_layer(input_ids)  # (1, L, D)

        block_indices, weights = self.router.get_predictive_indices(x, pool_input=True)
        
        # Asenkron prefetching: Tahmin edilen bloklarÄ± arka planda yÃ¼kle
        if prefetch and self._lazy_load:
            self.prefetch_blocks(block_indices)
        
        return block_indices, weights

    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        use_cache: bool = False,  # Zorunlu olarak False (KV Cache henÃ¼z desteklenmiyor)
        current_token_idx: Optional[int] = None,  # Sticky routing iÃ§in: ÅŸu anki token indeksi
    ) -> dict:
        """
        Forward pass: Sadece router'Ä±n seÃ§tiÄŸi bloklar Ã§alÄ±ÅŸÄ±r.
        Sticky Routing: EÄŸer sticky blocks varsa, router'Ä± atla ve sabitlenmiÅŸ bloklarÄ± kullan.

        Args:
            input_ids: (B, L) token ids
            attention_mask: (B, L) attention mask (opsiyonel)
            use_cache: KV cache kullan (opsiyonel, ÅŸimdilik False zorunlu)
            current_token_idx: Åu anki token indeksi (sticky routing iÃ§in)

        Returns:
            logits, hidden_states, selected_indices, router_info
        """
        # KRÄ°TÄ°K: KV Cache henÃ¼z desteklenmiyor (TupleCleaner past_key_values'i kaybediyor)
        # Åimdilik use_cache=False zorunlu tutuyoruz
        use_cache = False
        
        self.eval()
        B, L = input_ids.shape

        # Embedding
        x = self.embed_layer(input_ids)  # (B, L, D)
        
        # KRÄ°TÄ°K: Qwen2 katmanlarÄ± position_embeddings'i ZORUNLU olarak bekler
        # position_embeddings = (cos, sin) tuple'Ä± â€” RoPE iÃ§in gerekli
        # Bu olmadan TypeError: cannot unpack non-iterable NoneType object hatasÄ± alÄ±nÄ±r
        position_embeddings = None
        position_ids = None
        
        if self._is_qwen:
            try:
                # Position IDs oluÅŸtur
                position_ids = torch.arange(L, dtype=torch.long, device=input_ids.device)
                position_ids = position_ids.unsqueeze(0).expand(B, -1)
                
                # Rotary embedding kaynaÄŸÄ±nÄ± bul (birden fazla fallback)
                rotary_emb = self._rotary_emb
                
                # Fallback 1: model.model.rotary_emb (yeni transformers >= 4.38)
                if rotary_emb is None and self.model is not None:
                    try:
                        if hasattr(self.model, 'model') and hasattr(self.model.model, 'rotary_emb'):
                            rotary_emb = self.model.model.rotary_emb
                    except Exception:
                        pass
                
                # Fallback 2: layers[0].self_attn.rotary_emb (eski transformers)
                if rotary_emb is None and self.layers is not None and len(self.layers) > 0:
                    try:
                        first_layer = self.layers[0]
                        if hasattr(first_layer, 'self_attn') and hasattr(first_layer.self_attn, 'rotary_emb'):
                            rotary_emb = first_layer.self_attn.rotary_emb
                    except Exception:
                        pass
                
                if rotary_emb is not None:
                    # Rotary embeddings'den (cos, sin) hesapla
                    # FarklÄ± transformers versiyonlarÄ± farklÄ± imza kullanÄ±r
                    cos, sin = None, None
                    
                    # YÃ¶ntem 1: rotary_emb(x, position_ids) â€” yeni transformers
                    try:
                        result = rotary_emb(x, position_ids)
                        if isinstance(result, tuple) and len(result) == 2:
                            cos, sin = result
                    except Exception:
                        pass
                    
                    # YÃ¶ntem 2: rotary_emb(position_ids, seq_len=L) â€” eski transformers
                    if cos is None:
                        try:
                            cos, sin = rotary_emb(position_ids, seq_len=L)
                        except Exception:
                            pass
                    
                    # YÃ¶ntem 3: rotary_emb(position_ids) â€” bazÄ± versiyonlar
                    if cos is None:
                        try:
                            cos, sin = rotary_emb(position_ids)
                        except Exception:
                            pass
                    
                    if cos is not None and sin is not None:
                        # Device kontrolÃ¼
                        if cos.device != input_ids.device:
                            cos = cos.to(input_ids.device)
                        if sin.device != input_ids.device:
                            sin = sin.to(input_ids.device)
                        position_embeddings = (cos, sin)
                    else:
                        if not getattr(self, '_rotary_warn_printed', False):
                            print("âš ï¸  Rotary embeddings hesaplanamadÄ± â€” tÃ¼m yÃ¶ntemler baÅŸarÄ±sÄ±z")
                            self._rotary_warn_printed = True
                else:
                    if not getattr(self, '_rotary_warn_printed', False):
                        print("âš ï¸  Rotary embedding bulunamadÄ± (ne _rotary_emb ne de katmanlarda)")
                        self._rotary_warn_printed = True
            except Exception as e:
                print(f"âš ï¸  Position embeddings hesaplanamadÄ±: {e}")
                import traceback
                traceback.print_exc()

        # NO-SHARDING MODU: Router'Ä± atla, tÃ¼m katmanlarÄ± tek blokta Ã§alÄ±ÅŸtÄ±r
        if self.no_sharding:
            selected_indices = [0]  # Tek blok: Block 0
            weights = torch.ones(1)
            probs = None
            aux_loss = None
        # SEQUENTIAL LAZY MODU: TÃ¼m bloklarÄ± sÄ±rayla Ã§alÄ±ÅŸtÄ±r (VRAM tasarrufu + kalite)
        # Transformer katmanlarÄ± sÄ±ralÄ±dÄ±r â€” blok atlamak kalite kaybÄ±na neden olur
        # Bu mod: yÃ¼kle â†’ Ã§alÄ±ÅŸtÄ±r â†’ bellekten sil â€” tÃ¼m bloklar Ã§alÄ±ÅŸÄ±r ama sadece 1-2 bellekte
        elif getattr(self, '_sequential_all', False):
            selected_indices = list(range(self.num_blocks))
            weights = torch.ones(self.num_blocks) / self.num_blocks
            probs = None
            aux_loss = None
        # STICKY ROUTING: EÄŸer sticky blocks varsa ve henÃ¼z sÃ¼resi dolmamÄ±ÅŸsa, router'Ä± atla
        elif self._sticky_blocks is not None and current_token_idx is not None:
            if current_token_idx < self._sticky_until_token:
                # Sticky blocks'u kullan, router'Ä± Ã§alÄ±ÅŸtÄ±rma
                selected_indices = list(self._sticky_blocks)
                # Sticky blocks iÃ§in dummy weights (router Ã§alÄ±ÅŸmadÄ±ÄŸÄ± iÃ§in)
                weights = torch.ones(len(selected_indices)) / len(selected_indices)
                probs = None
                aux_loss = None
            else:
                # Sticky sÃ¼resi doldu, router'Ä± tekrar Ã§alÄ±ÅŸtÄ±r ve yeni bloklar seÃ§
                self._sticky_blocks = None
                self._sticky_until_token = 0
                # Router ile blok seÃ§imi
                probs, indices, aux_loss, weights = self.router(x, pool_input=True)
                indices = indices.squeeze().cpu().tolist()
                if isinstance(indices, int):
                    indices = [indices]
                selected_indices = indices[: self.top_k]
        else:
            # Normal router ile blok seÃ§imi
            probs, indices, aux_loss, weights = self.router(x, pool_input=True)
            indices = indices.squeeze().cpu().tolist()
            if isinstance(indices, int):
                indices = [indices]
            selected_indices = indices[: self.top_k]

        # TÃœM bloklarÄ± sÄ±rayla Ã§alÄ±ÅŸtÄ±r:
        # - SeÃ§ilen bloklar: Full hesaplama (VRAM kullanÄ±r)
        # - SeÃ§ilmeyen bloklar: Identity/residual geÃ§iÅŸ (sÄ±fÄ±r VRAM, sÄ±fÄ±r hesaplama)
        # Bu sayede transformer'Ä±n residual stream'i kesintisiz devam eder.
        # Layer dropping mantÄ±ÄŸÄ±: output = input (bloÄŸun katkÄ±sÄ± atlanÄ±r, ama akÄ±ÅŸ kopmaz)
        x_out = x
        selected_set = set(selected_indices)
        
        for idx in range(self.num_blocks):
            if idx in selected_set:
                # â”€â”€ SEÃ‡Ä°LEN BLOK: Full hesaplama â”€â”€
                if self._lazy_load:
                    with self._prefetch_lock:
                        if idx in self._loaded_blocks:
                            block = self._loaded_blocks[idx]
                        else:
                            block = self._load_block_from_disk(idx)
                else:
                    block = self.blocks[idx]
                
                if isinstance(block, QwenBlockWrapper) and position_embeddings is not None:
                    block_out = block(x_out, position_embeddings=position_embeddings, 
                                     position_ids=position_ids, 
                                     attention_mask=attention_mask)
                else:
                    block_out = block(x_out)
                
                # DEFANSÄ°F KODLAMA: Tuple kontrolÃ¼
                if isinstance(block_out, tuple):
                    x_out = block_out[0]
                elif hasattr(block_out, 'last_hidden_state'):
                    x_out = block_out.last_hidden_state
                elif isinstance(block_out, torch.Tensor):
                    x_out = block_out
                else:
                    try:
                        x_out = block_out[0] if hasattr(block_out, '__getitem__') else block_out
                    except Exception:
                        x_out = block_out
                
                while isinstance(x_out, (tuple, list)) and len(x_out) > 0:
                    x_out = x_out[0]
                
                if not isinstance(x_out, torch.Tensor):
                    raise TypeError(f"Blok {idx} Ã§Ä±ktÄ±sÄ± Tensor deÄŸil: {type(x_out)}")
            else:
                # â”€â”€ SEÃ‡Ä°LMEYEN BLOK: Identity (residual geÃ§iÅŸ) â”€â”€
                # x_out = x_out  â†’  BloÄŸun katkÄ±sÄ± atlanÄ±r ama akÄ±ÅŸ kopmaz
                pass

        # Final norm'dan Ã¶nce tuple kontrolÃ¼
        if isinstance(x_out, tuple):
            x_out = x_out[0]
        elif hasattr(x_out, 'last_hidden_state'):
            x_out = x_out.last_hidden_state
        
        # Final norm ve LM head (model'e gÃ¶re deÄŸiÅŸir)
        # Lazy loading durumunda model yok, final norm ve lm_head kaydedilmiÅŸ olmalÄ±
        if self.model is not None:
            if hasattr(self.model, "model") and hasattr(self.model.model, "norm"):
                x_out = self.model.model.norm(x_out)
                logits = self.model.lm_head(x_out)
            elif hasattr(self.model, "transformer") and hasattr(self.model.transformer, "ln_f"):
                x_out = self.model.transformer.ln_f(x_out)
                logits = self.model.lm_head(x_out)
            else:
                # Basit fallback
                logits = self.model.lm_head(x_out) if hasattr(self.model, "lm_head") else None
        else:
            # Lazy loading: kaydedilmiÅŸ final norm ve lm_head kullan
            if self._final_norm is not None:
                x_out = self._final_norm(x_out)
            if self._lm_head is not None:
                logits = self._lm_head(x_out)
            else:
                logits = None
                print("âš ï¸  Lazy loading: LM head bulunamadÄ±")

        # Router weights'i hazÄ±rla
        weights_cpu = weights.squeeze().cpu()
        if weights_cpu.dim() == 0:
            weights_cpu = weights_cpu.unsqueeze(0)
        weights_list = weights_cpu[: len(selected_indices)].tolist()

        # Blok kullanÄ±m takibini gÃ¼ncelle
        for idx in selected_indices:
            self._block_usage_count[idx] = self._block_usage_count.get(idx, 0) + 1
        
        # Lazy loading: KullanÄ±lmayan bloklarÄ± RAM'den kaldÄ±r (daha az agresif)
        # KRÄ°TÄ°K: Sadece gerÃ§ekten uzun sÃ¼re kullanÄ±lmayan bloklarÄ± sil
        # Thrashing'i Ã¶nlemek iÃ§in cleanup'Ä± daha az agresif yapÄ±yoruz
        if self._lazy_load and len(self._loaded_blocks) > (self.top_k * 2) + len(self._locked_blocks):
            # Kilitli bloklarÄ± ve seÃ§ili bloklarÄ± hariÃ§ tut
            unused_blocks = set(self._loaded_blocks.keys()) - set(selected_indices) - self._locked_blocks
            
            # En az kullanÄ±lan bloklarÄ± bul (kullanÄ±m sayÄ±sÄ±na gÃ¶re)
            unused_with_counts = [
                (idx, self._block_usage_count.get(idx, 0))
                for idx in unused_blocks
            ]
            unused_with_counts.sort(key=lambda x: x[1])  # En az kullanÄ±lan Ã¶nce
            
            # Sadece en az kullanÄ±lan birkaÃ§ bloÄŸu kaldÄ±r (thrashing'i Ã¶nlemek iÃ§in)
            blocks_to_remove = max(1, len(unused_blocks) - self.top_k)
            for unused_idx, _ in unused_with_counts[:blocks_to_remove]:
                self._unload_block_from_memory(unused_idx)
                # KullanÄ±m sayacÄ±nÄ± sÄ±fÄ±rla (kaldÄ±rÄ±ldÄ±ÄŸÄ± iÃ§in)
                self._block_usage_count.pop(unused_idx, None)
        
        return {
            "logits": logits,
            "hidden_states": x_out,
            "selected_indices": selected_indices,
            "router_weights": weights_list,
            "router_info": self.router.get_stats(),
            "blocks_in_memory": list(self._loaded_blocks.keys()) if self._lazy_load else None,
        }

    @torch.no_grad()
    def generate(
        self,
        prompt: str,
        max_new_tokens: int = 50,
        temperature: float = 0.8,
        top_k: int = 40,
        prefetch_next: bool = True,
    ) -> str:
        """
        Metin Ã¼retimi: Her adÄ±mda router hangi bloklarÄ± kullanacaÄŸÄ±na karar verir.
        Asenkron prefetching ile bir sonraki adÄ±mÄ±n bloklarÄ± Ã¶nceden yÃ¼klenir.

        Args:
            prompt: BaÅŸlangÄ±Ã§ metni
            max_new_tokens: Ãœretilecek maksimum token sayÄ±sÄ±
            temperature: Sampling sÄ±caklÄ±ÄŸÄ±
            top_k: Top-K sampling
            prefetch_next: True ise bir sonraki adÄ±mÄ±n bloklarÄ±nÄ± Ã¶nceden yÃ¼kle

        Returns:
            ÃœretilmiÅŸ metin
        """
        self.eval()
        input_ids = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)
        generated = input_ids

        # Ä°lk bloklarÄ± prefetch et
        if prefetch_next and self._lazy_load:
            first_indices, _ = self.predict_blocks(prompt, prefetch=True)

        # STICKY ROUTING: Ä°lk prompt iÃ§in router Ã§alÄ±ÅŸÄ±r ve bloklarÄ± sabitler
        initial_prompt_len = generated.shape[1]
        
        for step in range(max_new_tokens):
            current_token_idx = initial_prompt_len + step
            
            # Qwen2 gibi modern modeller causal masking'i kendi iÃ§inde halleder
            # DÄ±ÅŸarÄ±dan attention_mask gÃ¶ndermek dtype uyumsuzluÄŸuna neden olabilir
            # (SDPA bool/float bekler, long gÃ¶nderirsek RuntimeError)
            
            # Her adÄ±mda TÃœM baÄŸlamÄ± forward'a gÃ¶nder (KV Cache olmadÄ±ÄŸÄ± iÃ§in)
            outputs = self.forward(generated, attention_mask=None, use_cache=False, current_token_idx=current_token_idx)
            
            # EÄŸer sticky blocks yoksa veya sÃ¼resi dolduysa, yeni bloklarÄ± sabitle
            # NO-SHARDING modunda sticky routing yok (zaten tek blok var)
            if not self.no_sharding and self._sticky_blocks is None:
                selected_indices = outputs["selected_indices"]
                self._sticky_blocks = set(selected_indices)
                self._sticky_until_token = current_token_idx + self._sticky_duration
                print(f"ğŸ”’ Sticky Routing: Bloklar {self._sticky_blocks} sabitlendi (token {current_token_idx}-{self._sticky_until_token})")
            
            # Son token'Ä±n logits'ini al (tÃ¼m baÄŸlam Ã¼zerinden)
            logits = outputs["logits"][:, -1, :] / temperature

            # KRÄ°TÄ°K: Tokenizer vocab_size'dan bÃ¼yÃ¼k token ID'leri clamp et
            # Model 152064 token bekliyor ama tokenizer sadece 151643 token biliyor
            # EÄŸer model tokenizer'Ä±n vocab_size'Ä±ndan bÃ¼yÃ¼k bir ID Ã¼retirse, clamp et
            tokenizer_vocab_size = None
            if hasattr(self.tokenizer, 'vocab_size'):
                tokenizer_vocab_size = self.tokenizer.vocab_size
            elif hasattr(self.tokenizer, 'get_vocab'):
                tokenizer_vocab_size = len(self.tokenizer.get_vocab())
            
            if tokenizer_vocab_size and logits.size(-1) > tokenizer_vocab_size:
                # Logits'in son kÄ±smÄ±nÄ± -inf yap (tokenizer'Ä±n bilmediÄŸi token'lar)
                logits[:, tokenizer_vocab_size:] = float("-inf")
                if step == 0:  # Sadece ilk adÄ±mda yazdÄ±r
                    print(f"   âš ï¸  Token ID clamp: Logits {logits.size(-1)} â†’ {tokenizer_vocab_size} (tokenizer vocab_size) [sonraki adÄ±mlarda sessiz]")

            # Top-K sampling
            topk_vals, _ = torch.topk(logits, min(top_k, logits.size(-1)))
            logits[logits < topk_vals[:, -1:]] = float("-inf")
            probs = torch.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, 1)
            
            # KRÄ°TÄ°K: Ãœretilen token ID'sini tokenizer vocab_size'a clamp et
            if tokenizer_vocab_size:
                next_token = torch.clamp(next_token, 0, tokenizer_vocab_size - 1)
            
            # Yeni token'Ä± baÄŸlama ekle (bir sonraki adÄ±m iÃ§in)
            generated = torch.cat([generated, next_token], dim=1)

            # Prefetching: Sticky routing aktifken prefetch'e gerek yok
            # (Bloklar zaten sabitlenmiÅŸ ve RAM'de)
            if prefetch_next and self._lazy_load and step < max_new_tokens - 1:
                # Sadece sticky sÃ¼resi dolmak Ã¼zereyse bir sonraki adÄ±mÄ±n bloklarÄ±nÄ± tahmin et
                if current_token_idx >= self._sticky_until_token - 5:  # 5 token Ã¶nceden tahmin et
                    next_prompt = self.tokenizer.decode(generated[0].tolist(), skip_special_tokens=True)
                    next_indices, _ = self.predict_blocks(next_prompt, prefetch=True)

        result = self.tokenizer.decode(generated[0].tolist(), skip_special_tokens=True)
        
        # Sticky blocks'u temizle (bir sonraki generate iÃ§in)
        self._sticky_blocks = None
        self._sticky_until_token = 0
        
        return result

    def estimate_vram_savings(self) -> dict:
        """
        VRAM tasarrufu tahmini: TÃ¼m model vs. sadece top_k blok.
        """
        total_params = sum(p.numel() for p in self.model.parameters())
        block_params = sum(p.numel() for p in self.blocks[0].parameters())
        router_params = sum(p.numel() for p in self.router.parameters())

        # TÃ¼m model (float32)
        total_vram = total_params * 4 / (1024**3)  # GB

        # Sadece top_k blok + router
        sparse_vram = (block_params * self.top_k + router_params) * 4 / (1024**3)

        return {
            "total_params": total_params,
            "total_vram_gb": total_vram,
            "sparse_vram_gb": sparse_vram,
            "savings_ratio": total_vram / max(sparse_vram, 1e-9),
            "blocks_loaded": f"{self.top_k}/{self.num_blocks}",
        }

    def save_blocks_to_disk(self, save_dir: str):
        """
        Modeli bloklara ayÄ±rÄ±p diske kaydet (Sharding).
        Her blok ayrÄ± bir dosya olarak kaydedilir: block_0.pt, block_1.pt, ...
        
        Args:
            save_dir: BloklarÄ±n kaydedileceÄŸi dizin
        """
        import os
        os.makedirs(save_dir, exist_ok=True)
        
        print(f"ğŸ’¾ Model bloklarÄ± diske kaydediliyor: {save_dir}")
        
        for i, block in enumerate(self.blocks):
            block_path = os.path.join(save_dir, f"block_{i}.pt")
            
            # KRÄ°TÄ°K: Tam modÃ¼lÃ¼ kaydet (state_dict DEÄÄ°L!)
            # state_dict sadece sayÄ±larÄ± kaydeder, modÃ¼l yapÄ±sÄ±nÄ± kaybeder
            # torch.save(module) ise gerÃ§ek Qwen2DecoderLayer'larÄ± korur
            # BÃ¶ylece diskten yÃ¼klenince forward pass gerÃ§ek hesaplama yapar
            block_cpu = block.cpu()
            torch.save(block_cpu, block_path)
            block.to(self.device)  # Geri GPU'ya taÅŸÄ± (sharding modu iÃ§in)
            
            num_layers = len(block.layers) if isinstance(block, QwenBlockWrapper) else len(list(block.children()))
            block_size_mb = os.path.getsize(block_path) / (1024**2)
            print(f"   Blok {i}: {block_size_mb:.2f} MB ({num_layers} layer) â†’ {block_path}")
        
        # Final norm ve LM head'i de kaydet
        final_norm_state = None
        lm_head_state = None
        norm_type = None  # RMSNorm vs LayerNorm
        
        if hasattr(self.model, "model") and hasattr(self.model.model, "norm"):
            final_norm_state = self.model.model.norm.state_dict()
            lm_head_state = self.model.lm_head.state_dict()
            norm_type = type(self.model.model.norm).__name__
        elif hasattr(self.model, "transformer") and hasattr(self.model.transformer, "ln_f"):
            final_norm_state = self.model.transformer.ln_f.state_dict()
            lm_head_state = self.model.lm_head.state_dict()
            norm_type = type(self.model.transformer.ln_f).__name__
        
        # Rotary embeddings'i AYRI DOSYA olarak kaydet (lazy loading iÃ§in KRÄ°TÄ°K)
        # NOT: Yeni transformers'ta state_dict() boÅŸ {} dÃ¶nÃ¼yor (parametresiz)
        # Bu yÃ¼zden modÃ¼lÃ¼ doÄŸrudan kaydediyoruz
        if self._rotary_emb is not None:
            rotary_path = os.path.join(save_dir, "rotary_emb.pt")
            torch.save(self._rotary_emb, rotary_path)
            print(f"   Rotary embeddings kaydedildi: {type(self._rotary_emb).__name__} â†’ rotary_emb.pt")
        
        # Router, embedding ve metadata'yÄ± kaydet
        router_path = os.path.join(save_dir, "router.pt")
        torch.save({
            'router_state_dict': self.router.state_dict(),
            'embed_state_dict': self.embed_layer.state_dict(),
            'final_norm_state_dict': final_norm_state,
            'lm_head_state_dict': lm_head_state,
            'config': {
                'num_blocks': self.num_blocks,
                'top_k': self.top_k,
                'embed_dim': self.embed_dim,
                'layers_per_block': self.layers_per_block,
                'has_final_norm': final_norm_state is not None,
                'is_qwen': self._is_qwen,
                'norm_type': norm_type,
                'model_class': type(self.model).__name__,
                'has_rotary_emb': self._rotary_emb is not None,
            }
        }, router_path)
        
        print(f"   Router + Embedding + Final Norm + LM Head: {os.path.getsize(router_path) / (1024**2):.2f} MB")
        print(f"   Model tipi: {type(self.model).__name__} (is_qwen={self._is_qwen})")
        print(f"âœ… Toplam {self.num_blocks} blok kaydedildi")

    @classmethod
    def from_disk_blocks(
        cls,
        tokenizer,
        save_dir: str,
        device: str = "auto",
        lazy_load: bool = True,
        sticky_duration: int = 25,
        sequential_all: bool = True,  # True: tÃ¼m bloklar sÄ±rayla Ã§alÄ±ÅŸÄ±r (kaliteli), False: router seÃ§er (hÄ±zlÄ±)
    ):
        """
        Diskten bloklarÄ± yÃ¼kleyerek loader oluÅŸtur (Lazy Loading).
        
        Args:
            tokenizer: HuggingFace tokenizer
            save_dir: BloklarÄ±n kaydedildiÄŸi dizin
            device: 'auto', 'cuda', 'cpu'
            lazy_load: True ise bloklar RAM'e yÃ¼klenmez, sadece gerektiÄŸinde yÃ¼klenir
        
        Returns:
            HuggingFaceBlockLoader instance (bloklar diskte, RAM'de deÄŸil)
        """
        import os
        
        # Router config'i yÃ¼kle
        router_path = os.path.join(save_dir, "router.pt")
        router_data = torch.load(router_path, map_location='cpu')
        config = router_data['config']
        
        if device == "auto":
            device_obj = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            device_obj = torch.device(device)
        
        # Router ve embedding oluÅŸtur
        router = ExternalParisiNashRouter(
            embed_dim=config['embed_dim'],
            num_blocks=config['num_blocks'],
            top_k=config['top_k'],
        )
        router.load_state_dict(router_data['router_state_dict'])
        router.to(device_obj)
        
        # Embedding iÃ§in dummy model gerekli (sadece embed_layer iÃ§in)
        # Checkpoint'ten gerÃ§ek aÄŸÄ±rlÄ±k boyutunu alalÄ±m
        # Tokenizer'dan vocab_size almak yerine, checkpoint'teki gerÃ§ek boyutu kullan
        embed_dim = config['embed_dim']
        embed_weight = router_data['embed_state_dict']['weight']
        embed_vocab_size = embed_weight.shape[0]  # Checkpoint'teki gerÃ§ek vocab_size (Ã¶rn: 152064)
        
        embed_layer = nn.Embedding(embed_vocab_size, embed_dim)
        embed_layer.load_state_dict(router_data['embed_state_dict'])
        embed_layer.to(device_obj)
        
        # Final norm ve LM head'i yÃ¼kle (lazy loading iÃ§in)
        final_norm = None
        lm_head = None
        
        # Final norm yÃ¼kle (eÄŸer varsa)
        if config.get('has_final_norm', False):
            if router_data.get('final_norm_state_dict'):
                # Diskteki aÄŸÄ±rlÄ±klara bakalÄ±m: Bias var mÄ±?
                # Qwen ve bazÄ± yeni Llama modelleri bias'sÄ±z LayerNorm kullanÄ±r
                has_bias = 'bias' in router_data['final_norm_state_dict']
                
                # LayerNorm'u diskteki yapÄ±ya gÃ¶re oluÅŸtur
                final_norm = nn.LayerNorm(embed_dim, bias=has_bias)
                final_norm.load_state_dict(router_data['final_norm_state_dict'])
                final_norm.to(device_obj)
        
        # LM head yÃ¼kle (has_final_norm kontrolÃ¼nden baÄŸÄ±msÄ±z)
        if router_data.get('lm_head_state_dict'):
            # LM head iÃ§in de checkpoint'teki gerÃ§ek vocab_size'Ä± kullan
            lm_head_weight = router_data['lm_head_state_dict']['weight']
            lm_head_vocab_size = lm_head_weight.shape[0]  # Checkpoint'teki gerÃ§ek vocab_size
            
            # KRÄ°TÄ°K: Embedding ve LM Head vocab_size'larÄ±nÄ±n eÅŸleÅŸtiÄŸinden emin ol
            if embed_vocab_size != lm_head_vocab_size:
                raise ValueError(
                    f"Vocab size mismatch: Embedding={embed_vocab_size}, LM Head={lm_head_vocab_size}. "
                    f"Bu durum Ã§Ä±ktÄ± karakter bozulmasÄ±na neden olur. "
                    f"Ã–rnek: Model 'A' demek isterken 'ç„˜' (Ã‡ince karakter) basabilir. "
                    f"Checkpoint'i kontrol edin veya modeli yeniden kaydedin."
                )
            
            # Tokenizer vocab_size kontrolÃ¼ (opsiyonel ama Ã¶nerilir)
            tokenizer_vocab_size = None
            if hasattr(tokenizer, 'vocab_size'):
                tokenizer_vocab_size = tokenizer.vocab_size
            elif hasattr(tokenizer, 'get_vocab'):
                tokenizer_vocab_size = len(tokenizer.get_vocab())
            
            if tokenizer_vocab_size is not None:
                if embed_vocab_size != tokenizer_vocab_size:
                    print(f"\nâš ï¸  KRÄ°TÄ°K UYARI: Tokenizer vocab_size ({tokenizer_vocab_size}) != Model vocab_size ({embed_vocab_size})")
                    print(f"   Bu durum karakter kaymasÄ± (offset) hatalarÄ±na neden olur.")
                    print(f"   Ã–rnek: Model 'A' demek isterken 'ç„˜' (Ã‡ince karakter) basabilir.")
                    print(f"\n   ğŸ” Analiz:")
                    offset = embed_vocab_size - tokenizer_vocab_size
                    print(f"   - Offset: {offset} token (Model daha bÃ¼yÃ¼k)")
                    print(f"   - Tokenizer: Token ID'leri 0-{tokenizer_vocab_size-1} arasÄ±")
                    print(f"   - Model: Token ID'leri 0-{embed_vocab_size-1} arasÄ± bekliyor")
                    print(f"\n   ğŸ’¡ Ã‡Ã¶zÃ¼m:")
                    print(f"   - Model checkpoint'teki vocab_size kullanÄ±lacak: {embed_vocab_size}")
                    print(f"   - Tokenizer'Ä±n token ID'leri model'in embedding matrisine uyacak ÅŸekilde ayarlanmalÄ±")
                    print(f"   - EÄŸer tokenizer'Ä±n ID'leri model'in beklediÄŸi aralÄ±ÄŸÄ±n dÄ±ÅŸÄ±ndaysa, IndexError oluÅŸabilir")
                    print(f"\n   ğŸ§ª Test: no_sharding=True ile test edin")
                    print(f"   - Ã‡alÄ±ÅŸÄ±yorsa: Sorun sharding'de")
                    print(f"   - Hala bozuksa: Sorun vocab mapping offset'inde")
                else:
                    print(f"âœ… Tokenizer ve Model vocab_size eÅŸleÅŸiyor: {embed_vocab_size}")
            
            lm_head = nn.Linear(embed_dim, lm_head_vocab_size, bias=False)
            lm_head.load_state_dict(router_data['lm_head_state_dict'])
            lm_head.to(device_obj)
        
        # Lazy loader oluÅŸtur
        loader = cls.__new__(cls)
        
        # KRÄ°TÄ°K: PyTorch modÃ¼l yapÄ±sÄ±nÄ± baÅŸlat (nn.Module.__init__ Ã§aÄŸrÄ±sÄ±)
        nn.Module.__init__(loader)
        
        loader.tokenizer = tokenizer
        loader.num_blocks = config['num_blocks']
        loader.top_k = config['top_k']
        loader.device = device_obj
        loader.embed_dim = embed_dim
        loader.layers_per_block = config['layers_per_block']
        loader.router = router
        loader.embed_layer = embed_layer
        loader._final_norm = final_norm
        loader._lm_head = lm_head
        
        # Model tipi bilgisi (Qwen desteÄŸi iÃ§in)
        loader._is_qwen = config.get('is_qwen', False)
        loader.no_sharding = False
        loader.model = None  # Lazy loading'de model RAM'de deÄŸil
        loader.layers = None  # Lazy loading'de layers yok
        
        # Sequential lazy mod: TÃ¼m bloklar sÄ±rayla Ã§alÄ±ÅŸÄ±r
        # Router eÄŸitimsizken bu mod ZORUNLU (aksi halde Ã§Ã¶p Ã§Ä±ktÄ±)
        loader._sequential_all = sequential_all
        if sequential_all:
            print(f"âœ… Sequential lazy mod aktif: TÃ¼m {config['num_blocks']} blok sÄ±rayla Ã§alÄ±ÅŸacak")
            print(f"   VRAM tasarrufu: Sadece 1-2 blok aynÄ± anda bellekte")
        
        # Rotary embeddings'i diskten yÃ¼kle (Qwen modelleri iÃ§in KRÄ°TÄ°K)
        # AyrÄ± dosya olarak kaydedildi: rotary_emb.pt
        loader._rotary_emb = None
        rotary_path = os.path.join(save_dir, "rotary_emb.pt")
        if os.path.exists(rotary_path):
            try:
                loader._rotary_emb = torch.load(rotary_path, map_location=device_obj, weights_only=False)
                loader._rotary_emb.eval()
                print(f"âœ… Rotary embeddings diskten yÃ¼klendi: {type(loader._rotary_emb).__name__}")
            except Exception as e:
                print(f"âš ï¸  Rotary embeddings yÃ¼klenemedi: {e}")
        elif loader._is_qwen:
            print(f"âš ï¸  rotary_emb.pt bulunamadÄ± â€” modeli tekrar save_blocks_to_disk ile kaydedin")
        
        # BloklarÄ± lazy yÃ¼kle (ÅŸimdilik boÅŸ, gerektiÄŸinde diskten yÃ¼klenecek)
        loader.blocks = nn.ModuleList()
        loader._block_paths = []
        loader._loaded_blocks = {}  # Cache: {block_idx: nn.Module}
        loader._lazy_load = lazy_load
        
        # KorumalÄ± bloklar (thrashing'i Ã¶nlemek iÃ§in)
        loader._locked_blocks = {0, 1}
        
        # Blok kullanÄ±m takibi (cleanup iÃ§in)
        loader._block_usage_count = {}
        
        # Sticky Routing
        loader._sticky_blocks = None
        loader._sticky_until_token = 0
        loader._sticky_duration = sticky_duration
        
        # Asenkron prefetching iÃ§in
        loader._prefetch_queue = Queue()
        loader._prefetch_thread = None
        loader._prefetch_running = False
        loader._prefetch_lock = threading.Lock()
        
        # Blok yollarÄ±nÄ± kaydet ve save_dir'Ä± sakla (blok yapÄ±sÄ±nÄ± yeniden oluÅŸturmak iÃ§in)
        loader._save_dir = save_dir
        
        for i in range(config['num_blocks']):
            block_path = os.path.join(save_dir, f"block_{i}.pt")
            loader._block_paths.append(block_path)
            if lazy_load:
                loader.blocks.append(nn.Identity())  # Placeholder
            else:
                # Eager: hemen yÃ¼kle (gerÃ§ek modÃ¼lÃ¼ torch.load ile)
                block = torch.load(block_path, map_location=device_obj, weights_only=False)
                block.eval()
                loader.blocks.append(block)
        
        loader.model = None
        loader.layers = None
        
        return loader

    def calibrate_router(
        self,
        calibration_prompts: list = None,
        num_steps: int = 100,
        lr: float = 1e-3,
    ):
        """
        Router'Ä± Knowledge Distillation ile eÄŸit.
        
        Strateji:
        1. Teacher: TÃ¼m bloklar sÄ±rayla â†’ referans logits
        2. Ablasyon: Her top_k blok kombinasyonu test edilir, KL divergence Ã¶lÃ§Ã¼lÃ¼r
        3. En iyi kombinasyon bulunur
        4. Router bu kombinasyonlarÄ± seÃ§meyi Ã¶ÄŸrenir
        """
        import itertools
        
        if calibration_prompts is None:
            calibration_prompts = [
                "The history of artificial intelligence is",
                "In mathematics, a prime number is",
                "The capital of France is Paris, which is known for",
                "def fibonacci(n):\n    if n <= 1:\n        return n",
                "Machine learning models can be trained using",
                "The theory of relativity states that",
                "To cook a perfect pasta, you need to",
                "The human brain contains approximately",
            ]
        
        print(f"\n{'='*60}")
        print(f"ğŸ“ ROUTER KALÄ°BRASYONU (Knowledge Distillation)")
        print(f"{'='*60}")
        print(f"   Blok: {self.num_blocks}, top_k: {self.top_k}")
        
        all_combos = list(itertools.combinations(range(self.num_blocks), self.top_k))
        print(f"   Test edilecek kombinasyon: {len(all_combos)}")
        
        # â”€â”€ ADIM 1: Teacher logits (tÃ¼m bloklar sÄ±rayla) â”€â”€
        print(f"\nğŸ“– AdÄ±m 1: Teacher (tÃ¼m {self.num_blocks} blok)...")
        teacher_data = []
        
        for prompt in calibration_prompts:
            input_ids = self.tokenizer.encode(prompt, return_tensors="pt").to(self.device)
            with torch.no_grad():
                x = self.embed_layer(input_ids)
                x_running = x.clone()
                for bi in range(self.num_blocks):
                    block = self._load_block_from_disk(bi) if self._lazy_load else self.blocks[bi]
                    x_running = self._run_single_block(block, x_running)
                t_logits = self._get_logits(x_running)
                teacher_data.append((x, t_logits))
        print(f"   âœ… {len(calibration_prompts)} teacher logits hazÄ±r")
        
        # â”€â”€ ADIM 2: Ablasyon (her kombinasyon) â”€â”€
        print(f"\nğŸ”¬ AdÄ±m 2: {len(all_combos)} kombinasyon test ediliyor...")
        combo_scores = {}
        
        for combo in all_combos:
            total_kl = 0.0
            combo_set = set(combo)
            for x_embed, t_logits in teacher_data:
                with torch.no_grad():
                    x_running = x_embed.clone()
                    # Forward ile aynÄ± mantÄ±k: seÃ§ilen â†’ full, diÄŸer â†’ identity
                    for bi in range(self.num_blocks):
                        if bi in combo_set:
                            block = self._load_block_from_disk(bi) if self._lazy_load else self.blocks[bi]
                            x_running = self._run_single_block(block, x_running)
                        # else: identity (pass)
                    s_logits = self._get_logits(x_running)
                    t_p = torch.nn.functional.softmax(t_logits.float(), dim=-1)
                    s_lp = torch.nn.functional.log_softmax(s_logits.float(), dim=-1)
                    kl = torch.nn.functional.kl_div(s_lp, t_p, reduction='batchmean')
                    total_kl += kl.item()
            combo_scores[combo] = total_kl / len(teacher_data)
        
        sorted_combos = sorted(combo_scores.items(), key=lambda x: x[1])
        print(f"\nğŸ“Š En Ä°yi 5 Kombinasyon:")
        for rank, (combo, kl) in enumerate(sorted_combos[:5]):
            m = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰", "  ", "  "][rank]
            print(f"   {m} Blok {list(combo)}: KL = {kl:.4f}")
        
        best_combo = sorted_combos[0][0]
        print(f"\n   ğŸ† En iyi: {list(best_combo)} (KL={sorted_combos[0][1]:.4f})")
        print(f"   âŒ En kÃ¶tÃ¼: {list(sorted_combos[-1][0])} (KL={sorted_combos[-1][1]:.4f})")
        
        # â”€â”€ ADIM 3: Router eÄŸitimi â”€â”€
        print(f"\nğŸ”§ AdÄ±m 3: Router eÄŸitimi ({num_steps} adÄ±m)...")
        
        # KESKÄ°N hedef daÄŸÄ±lÄ±m: En iyi 3 kombodan oluÅŸtur
        # Softmax(âˆ’KL / temperature) ile keskin aÄŸÄ±rlÄ±klandÄ±r
        kl_values = torch.tensor([kl for _, kl in sorted_combos], device=self.device)
        sharp_weights = torch.nn.functional.softmax(-kl_values / 5.0, dim=0)  # temperature=5 â†’ keskin
        
        target_dist = torch.zeros(self.num_blocks, device=self.device)
        for (combo, _), weight in zip(sorted_combos, sharp_weights):
            for bi in combo:
                target_dist[bi] += weight.item()
        target_dist = target_dist / target_dist.sum()
        print(f"   Hedef daÄŸÄ±lÄ±m: {[f'{t:.3f}' for t in target_dist.tolist()]}")
        
        # Her prompt iÃ§in en iyi komboyu bul â†’ prompt-specific hedefler
        prompt_targets = []
        for p_idx, (x_embed, t_logits) in enumerate(teacher_data):
            best_kl = float('inf')
            best_combo_for_prompt = best_combo
            for combo, kl_val in combo_scores.items():
                # Bu prompt iÃ§in KL'yi ayrÄ± hesapla
                with torch.no_grad():
                    x_r = x_embed.clone()
                    cs = set(combo)
                    for bi in range(self.num_blocks):
                        if bi in cs:
                            block = self._load_block_from_disk(bi) if self._lazy_load else self.blocks[bi]
                            x_r = self._run_single_block(block, x_r)
                    s_log = self._get_logits(x_r)
                    t_p = torch.nn.functional.softmax(t_logits.float(), dim=-1)
                    s_lp = torch.nn.functional.log_softmax(s_log.float(), dim=-1)
                    kl = torch.nn.functional.kl_div(s_lp, t_p, reduction='batchmean').item()
                    if kl < best_kl:
                        best_kl = kl
                        best_combo_for_prompt = combo
            
            # Bu prompt iÃ§in keskin hedef
            pt = torch.zeros(self.num_blocks, device=self.device)
            for bi in best_combo_for_prompt:
                pt[bi] = 0.5  # Her seÃ§ilen blok %50
            prompt_targets.append(pt)
        
        # Router'Ä± float32'ye al ve eÄŸit
        original_dtype = next(self.router.parameters()).dtype
        if original_dtype != torch.float32:
            self.router = self.router.float()
        self.router.train()
        
        # YÃ¼ksek lr + daha fazla adÄ±m
        optimizer = torch.optim.Adam(self.router.parameters(), lr=lr * 3)
        
        for step in range(num_steps):
            total_loss = 0.0
            for p_idx, (x_embed, _) in enumerate(teacher_data):
                probs, _, aux_loss, _ = self.router(x_embed.float(), pool_input=True)
                probs_sq = probs.squeeze()
                
                # Ä°ki loss: global hedef + prompt-specific hedef
                loss_global = torch.nn.functional.mse_loss(probs_sq, target_dist)
                loss_prompt = torch.nn.functional.mse_loss(probs_sq, prompt_targets[p_idx])
                loss = 0.3 * loss_global + 0.7 * loss_prompt + 0.001 * aux_loss
                
                total_loss += loss.item()
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
            if (step + 1) % 20 == 0:
                print(f"   AdÄ±m {step+1}/{num_steps}: Loss = {total_loss / len(teacher_data):.6f}")
        
        self.router.eval()
        if original_dtype != torch.float32:
            self.router = self.router.to(dtype=original_dtype)
        
        # â”€â”€ TEORÄ° ENTEGRASYONU â”€â”€
        # Router temperature'Ä±nÄ± dÃ¼ÅŸÃ¼r (Parisi tavlama son aÅŸamasÄ±)
        self.router.set_temperature(0.5)
        
        # Nash regret'i seed et: En iyi kombo bloklarÄ±na avantaj
        # Negatif regret = "bu blok yeterince seÃ§ildi" â†’ avantaj
        # Pozitif regret = "bu blok az seÃ§ildi" â†’ dezavantaj (bu durumda istemiyoruz)
        regret_seed = torch.zeros(self.num_blocks, device=self.device)
        usage_seed = torch.zeros(self.num_blocks, device=self.device)
        for bi in best_combo:
            regret_seed[bi] = 0.3   # Pozitif regret â†’ logit bonusu â†’ daha Ã§ok seÃ§ilir
            usage_seed[bi] = 0.4    # YÃ¼ksek usage â†’ sÃ¼rÃ¼ hafÄ±zasÄ±
        for bi in range(self.num_blocks):
            if bi not in best_combo:
                regret_seed[bi] = -0.1  # Negatif regret â†’ logit penalty
                usage_seed[bi] = 0.1
        self.router.cumulative_regret.copy_(regret_seed)
        self.router.block_usage.copy_(usage_seed)
        
        # Teori durumu raporu
        stats = self.router.get_stats()
        print(f"\nğŸ“ Teori Durumu:")
        ts = stats.get('theory_status', {})
        print(f"   Nash Regret: {'âœ… Aktif' if ts.get('nash_regret_active') else 'âŒ Pasif'}")
        print(f"   Parisi Tavlama: {'âœ… Aktif' if ts.get('parisi_annealing_active') else 'â¸ï¸  SoÄŸuma tamamlandÄ±'}")
        print(f"   SÄ±ÄŸÄ±rcÄ±k KeÅŸif: {'âœ… Aktif' if ts.get('starling_exploration_active') else 'âŒ Pasif'}")
        print(f"   Regret: {[f'{r:.3f}' for r in stats['regret'].tolist()]}")
        print(f"   Usage: {[f'{u:.3f}' for u in stats['usage'].tolist()]}")
        
        # SonuÃ§
        print(f"\nâœ… Kalibrasyon tamamlandÄ±!")
        test_prompt = calibration_prompts[0]
        idxs, wts = self.predict_blocks(test_prompt, prefetch=False)
        print(f"   Router seÃ§imi: {idxs} (en iyi: {list(best_combo)})")
        print(f"   AÄŸÄ±rlÄ±klar: {[f'{w:.2%}' for w in wts.tolist()]}")
        match = set(idxs) == set(best_combo)
        print(f"   {'âœ… EÅŸleÅŸti!' if match else 'âš ï¸  YakÄ±n ama tam eÅŸleÅŸmedi'}")
        
        # TÃ¼m promptlar iÃ§in kontrol
        match_count = 0
        for p_idx, prompt in enumerate(calibration_prompts):
            idxs_p, _ = self.predict_blocks(prompt, prefetch=False)
            pt_best = [i for i, v in enumerate(prompt_targets[p_idx]) if v > 0]
            if set(idxs_p) == set(pt_best):
                match_count += 1
        print(f"   DoÄŸruluk: {match_count}/{len(calibration_prompts)} prompt eÅŸleÅŸti")
        print(f"{'='*60}\n")
    
    def _run_single_block(self, block, x):
        """Tek bir bloÄŸu position_embeddings ile Ã§alÄ±ÅŸtÄ±r."""
        position_embeddings = None
        L = x.shape[1]
        position_ids = torch.arange(L, device=self.device).unsqueeze(0)
        if self._is_qwen and self._rotary_emb is not None:
            try:
                cos, sin = self._rotary_emb(x, position_ids)
                position_embeddings = (cos.to(self.device), sin.to(self.device))
            except Exception:
                pass
        if isinstance(block, QwenBlockWrapper) and position_embeddings is not None:
            out = block(x, position_embeddings=position_embeddings,
                        position_ids=position_ids, attention_mask=None)
        else:
            out = block(x)
        return out[0] if isinstance(out, tuple) else out
    
    def _get_logits(self, hidden_states):
        """Hidden states'ten logits hesapla (final_norm + lm_head)."""
        # Final norm
        if self.model is not None:
            if hasattr(self.model, "model") and hasattr(self.model.model, "norm"):
                x = self.model.model.norm(hidden_states)
            else:
                x = hidden_states
            # LM head
            if hasattr(self.model, "lm_head"):
                return self.model.lm_head(x)
        else:
            # Lazy loading
            if self._final_norm is not None:
                x = self._final_norm(hidden_states)
            else:
                x = hidden_states
            if self._lm_head is not None:
                return self._lm_head(x)
        return None

    def _load_block_from_disk(self, block_idx: int) -> nn.Module:
        """
        Diskten bir bloÄŸu yÃ¼kle (Lazy Loading) ve cache'e ekle.
        KRÄ°TÄ°K: torch.load ile GERÃ‡EK modÃ¼l yÃ¼klenir (QwenBlockWrapper + Qwen2DecoderLayer)
        
        Args:
            block_idx: YÃ¼klenecek blok indeksi
        
        Returns:
            YÃ¼klenmiÅŸ blok modÃ¼lÃ¼ (gerÃ§ek forward pass yapar)
        """
        if block_idx in self._loaded_blocks:
            return self._loaded_blocks[block_idx]
        
        if block_idx >= len(self._block_paths):
            raise ValueError(f"Blok {block_idx} bulunamadÄ±")
        
        block_path = self._block_paths[block_idx]
        print(f"ğŸ“‚ Diskten yÃ¼kleniyor: block_{block_idx}.pt")
        
        # KRÄ°TÄ°K: GerÃ§ek modÃ¼lÃ¼ yÃ¼kle (state_dict deÄŸil!)
        # torch.save(module) ile kaydedildi, torch.load ile gerÃ§ek Qwen2DecoderLayer geri gelir
        block = torch.load(block_path, map_location=self.device, weights_only=False)
        
        # KRÄ°TÄ°K: Dtype uyumu! block.cpu() float32'ye dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r ama model float16.
        # Embed layer'in dtype'Ä±na cast et
        try:
            embed_dtype = next(self.embed_layer.parameters()).dtype
            block = block.to(dtype=embed_dtype)
        except (StopIteration, AttributeError):
            pass
        
        block.eval()
        
        # Cache'e ekle
        self._loaded_blocks[block_idx] = block
        return block

    def _unload_block_from_memory(self, block_idx: int):
        """
        RAM'den bir bloÄŸu kaldÄ±r (bellek tasarrufu).
        KorumalÄ± bloklar (locked blocks) kaldÄ±rÄ±lmaz.
        
        Args:
            block_idx: KaldÄ±rÄ±lacak blok indeksi
        """
        # KorumalÄ± bloklarÄ± kontrol et (Block 0 varsayÄ±lan olarak kilitli)
        if block_idx in self._locked_blocks:
            print(f"ğŸ”’ Blok {block_idx} korumalÄ±, RAM'den kaldÄ±rÄ±lmÄ±yor")
            return
        
        if block_idx in self._loaded_blocks:
            del self._loaded_blocks[block_idx]
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            print(f"ğŸ—‘ï¸  Blok {block_idx} RAM'den kaldÄ±rÄ±ldÄ±")
    
    def lock_block(self, block_idx: int):
        """
        Bir bloÄŸu korumalÄ± yap (RAM'den kaldÄ±rÄ±lmasÄ±nÄ± engelle).
        
        Args:
            block_idx: Kilitlenecek blok indeksi
        """
        self._locked_blocks.add(block_idx)
        print(f"ğŸ”’ Blok {block_idx} kilitlendi (RAM'den kaldÄ±rÄ±lmayacak)")
    
    def unlock_block(self, block_idx: int):
        """
        Bir bloÄŸun kilidini kaldÄ±r (RAM'den kaldÄ±rÄ±labilir hale getir).
        
        Args:
            block_idx: Kilidi kaldÄ±rÄ±lacak blok indeksi
        """
        if block_idx in self._locked_blocks:
            self._locked_blocks.remove(block_idx)
            print(f"ğŸ”“ Blok {block_idx} kilidi kaldÄ±rÄ±ldÄ±")

    def _prefetch_worker(self):
        """
        Arka plan thread'i: Prefetch kuyruÄŸundaki bloklarÄ± yÃ¼kler.
        """
        while self._prefetch_running:
            try:
                block_idx = self._prefetch_queue.get(timeout=1.0)
                if block_idx is None:  # Shutdown signal
                    break
                
                # Blok zaten yÃ¼klÃ¼ mÃ¼ kontrol et
                with self._prefetch_lock:
                    if block_idx not in self._loaded_blocks:
                        self._load_block_from_disk(block_idx)
                
                self._prefetch_queue.task_done()
            except Exception:
                continue

    def start_prefetching(self, num_workers: int = 2):
        """
        Asenkron prefetching'i baÅŸlat.
        
        Args:
            num_workers: Paralel prefetch thread sayÄ±sÄ± (default: 2)
        """
        if not hasattr(self, '_prefetch_threads'):
            self._prefetch_threads = []
        
        # Ã‡alÄ±ÅŸan thread var mÄ± kontrol et
        alive = [t for t in self._prefetch_threads if t.is_alive()]
        if len(alive) >= num_workers:
            return
        
        self._prefetch_running = True
        for i in range(num_workers - len(alive)):
            t = threading.Thread(target=self._prefetch_worker, daemon=True)
            t.start()
            self._prefetch_threads.append(t)
        
        print(f"ğŸš€ Asenkron prefetching baÅŸlatÄ±ldÄ± ({num_workers} worker)")

    def stop_prefetching(self):
        """Asenkron prefetching'i durdur."""
        self._prefetch_running = False
        # Her worker iÃ§in shutdown signal
        for _ in range(len(getattr(self, '_prefetch_threads', []))):
            self._prefetch_queue.put(None)
        for t in getattr(self, '_prefetch_threads', []):
            t.join(timeout=2.0)
        self._prefetch_threads = []
        print("â¹ï¸  Asenkron prefetching durduruldu")

    def prefetch_blocks(self, block_indices: List[int]):
        """
        BloklarÄ± Ã¶nceden yÃ¼kle (asenkron).
        Router'Ä±n tahmin ettiÄŸi bloklarÄ± arka planda yÃ¼klemeye baÅŸlar.
        
        Args:
            block_indices: YÃ¼klenecek blok indeksleri listesi
        """
        if not self._lazy_load:
            return
        
        # Prefetching baÅŸlatÄ±lmamÄ±ÅŸsa baÅŸlat
        if not getattr(self, '_prefetch_threads', None) or not any(t.is_alive() for t in self._prefetch_threads):
            self.start_prefetching()
        
        # BloklarÄ± kuyruÄŸa ekle
        for idx in block_indices:
            if idx not in self._loaded_blocks:
                self._prefetch_queue.put(idx)
