"""
Parisi-Nash External Router: Qwen2.5-7B ile Dinamik Blok YÃ¼kleme
================================================================
Bu script, colab_demo_hf.ipynb notebook'unun Python script versiyonudur.
Google Colab veya yerel makinede Ã§alÄ±ÅŸtÄ±rÄ±labilir.

Ã–zellikler:
  âœ… SÄ±fÄ±r eÄŸitim maliyeti: Mevcut modeli yeniden eÄŸitmeden kullanÄ±r
  âœ… Dinamik RAM yÃ¶netimi: 7B parametre â†’ sadece 2B RAM'de
  âœ… Sticky Routing: Bloklar belirli bir sÃ¼re sabit kalÄ±r (thrashing Ã¶nleme)
  âœ… Lazy Loading: Bloklar diskten gerektiÄŸinde yÃ¼klenir
  âœ… Vocab Alignment: Tokenizer ve model vocab_size kontrolÃ¼
  âœ… GÃ¼venli Token YÃ¶netimi: Token aÃ§Ä±k metin olarak saklanmaz

KullanÄ±m:
    # Colab'da:
    !python colab_demo_hf_script.py
    
    # Yerel makinede:
    python colab_demo_hf_script.py --model Qwen/Qwen2.5-7B --num_blocks 7 --top_k 2
    
    # Sadece no-sharding testi:
    python colab_demo_hf_script.py --test-only
"""

import torch
import os
import sys
import argparse


def get_hf_token():
    """HuggingFace token'Ä±nÄ± gÃ¼venli bir ÅŸekilde al."""
    # 1. Environment variable
    token = os.environ.get('HF_TOKEN')
    if token:
        print("âœ… Token environment variable'dan alÄ±ndÄ±")
        return token
    
    # 2. Colab Secrets
    try:
        from google.colab import userdata
        token = userdata.get('HF_TOKEN')
        if token:
            print("âœ… Token Colab Secrets'tan alÄ±ndÄ±")
            return token
    except ImportError:
        pass
    
    # 3. Huggingface CLI login
    try:
        from huggingface_hub import HfFolder
        token = HfFolder.get_token()
        if token:
            print("âœ… Token huggingface-cli login'den alÄ±ndÄ±")
            return token
    except ImportError:
        pass
    
    # 4. Manuel giriÅŸ
    print("âš ï¸  HF_TOKEN bulunamadÄ±.")
    print("   SeÃ§enekler:")
    print("   1. export HF_TOKEN=hf_xxx... (terminal)")
    print("   2. huggingface-cli login (CLI)")
    print("   3. Colab Secrets'a ekle (Colab)")
    token = input("Token giriniz (veya Enter ile devam): ").strip()
    return token if token else None


def load_model(model_name: str, hf_token: str):
    """HuggingFace modelini yÃ¼kle."""
    from transformers import AutoTokenizer, AutoModelForCausalLM
    
    print(f"\nğŸš€ {model_name} yÃ¼kleniyor...")
    print("   âš ï¸  Bu iÅŸlem birkaÃ§ dakika sÃ¼rebilir...")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        token=hf_token,
        torch_dtype=torch.float16,
        device_map="auto",
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    total_params = sum(p.numel() for p in model.parameters()) / 1e9
    print(f"âœ… Model yÃ¼klendi: {total_params:.1f}B parametre")
    
    return model, tokenizer


def test_no_sharding(model, tokenizer, prompt: str):
    """NO-SHARDING test modu: Modeli hiÃ§ bÃ¶lmeden Ã§alÄ±ÅŸtÄ±r."""
    from swarm_llm.hf_loader import HuggingFaceBlockLoader
    
    print("\n" + "="*70)
    print("ğŸ”¬ NO-SHARDING TEST MODU")
    print("="*70)
    print("   EÄŸer bu modda Ã§alÄ±ÅŸÄ±yorsa: Sorun sharding'de")
    print("   EÄŸer hala bozuksa: Sorun model yÃ¼kleme veya tokenizer'da\n")
    
    loader = HuggingFaceBlockLoader(
        model=model,
        tokenizer=tokenizer,
        no_sharding=True,
        device="auto",
    )
    
    print(f"âœ… NO-SHARDING modu aktif")
    print(f"   Toplam layer: {len(loader.layers)}")
    print(f"   TÃ¼m katmanlar tek blokta: Block 0\n")
    
    print(f"ğŸ§ª Test Prompt: '{prompt}'")
    print(f"ğŸ”„ Metin Ã¼retimi baÅŸlÄ±yor (NO-SHARDING modu)...\n")
    
    generated = loader.generate(
        prompt=prompt,
        max_new_tokens=50,
        temperature=0.8,
        top_k=40,
    )
    
    print(f"\nğŸ“ Ãœretilen metin (NO-SHARDING):")
    print(f"'{generated}'\n")
    
    # SonuÃ§ analizi
    if "ç„˜" in generated or "æº¦" in generated or len(generated.split()) < 5:
        print("âŒ SONUÃ‡: NO-SHARDING modunda da sorun var!")
        print("   â†’ Sorun sharding'de DEÄÄ°L, model yÃ¼kleme veya tokenizer'da")
        return False
    else:
        print("âœ… SONUÃ‡: NO-SHARDING modunda Ã§alÄ±ÅŸÄ±yor!")
        return True


def run_sharded_demo(model, tokenizer, num_blocks: int, top_k: int, prompt: str, save_dir: str):
    """Normal sharding modu ile demo."""
    from swarm_llm.hf_loader import HuggingFaceBlockLoader
    
    print("\n" + "="*70)
    print(f"ğŸ”§ SHARDING MODU: {num_blocks} blok, top_k={top_k}")
    print("="*70)
    
    # Bloklara bÃ¶l
    loader = HuggingFaceBlockLoader(
        model=model,
        tokenizer=tokenizer,
        num_blocks=num_blocks,
        top_k=top_k,
        device="auto",
    )
    
    print(f"âœ… Model bloklara bÃ¶lÃ¼ndÃ¼")
    print(f"   Toplam layer: {len(loader.layers)}")
    print(f"   Blok sayÄ±sÄ±: {loader.num_blocks}")
    print(f"   Her blok: {loader.layers_per_block} layer")
    print(f"   Her forward'da: {loader.top_k}/{loader.num_blocks} blok Ã§alÄ±ÅŸÄ±r")
    
    # VRAM tasarrufu
    savings = loader.estimate_vram_savings()
    print(f"\nğŸ’¾ VRAM Tasarrufu:")
    print(f"   TÃ¼m model: {savings['total_vram_gb']:.1f} GB")
    print(f"   Seyrek ({savings['blocks_loaded']} blok): {savings['sparse_vram_gb']:.1f} GB")
    print(f"   Tasarruf: {savings['savings_ratio']:.1f}x")
    
    # Blok tahmini
    block_indices, weights = loader.predict_blocks(prompt, prefetch=False)
    print(f"\nğŸ”® Blok Tahmini:")
    print(f"   Prompt: '{prompt}'")
    print(f"   Tahmin edilen bloklar: {block_indices}")
    print(f"   AÄŸÄ±rlÄ±klar: {[f'{w:.2%}' for w in weights.tolist()]}")
    
    # Diske kaydet
    if save_dir:
        print(f"\nğŸ’¾ Bloklar diske kaydediliyor: {save_dir}")
        loader.save_blocks_to_disk(save_dir)
        
        # â”€â”€ HEMEN DRIVE'A KOPYALA (kalibrasyon Ã§Ã¶kmeden Ã¶nce!) â”€â”€
        try:
            from google.colab import drive
            import shutil
            print(f"\nğŸ“ Google Drive'a kopyalanÄ±yor...")
            drive.mount('/content/drive', force_remount=False)
            drive_target = f"/content/drive/MyDrive/swarm_model_blocks"
            os.makedirs(drive_target, exist_ok=True)
            
            files_copied = 0
            total_size_mb = 0
            for fname in sorted(os.listdir(save_dir)):
                if fname.endswith('.pt'):
                    src = os.path.join(save_dir, fname)
                    dst = os.path.join(drive_target, fname)
                    fsize_mb = os.path.getsize(src) / (1024**2)
                    print(f"   ğŸ“„ {fname} ({fsize_mb:.1f} MB) â†’ Drive")
                    shutil.copy2(src, dst)
                    files_copied += 1
                    total_size_mb += fsize_mb
            
            print(f"\nâœ… {files_copied} dosya Drive'a kopyalandÄ±!")
            print(f"   ğŸ“ My Drive/swarm_model_blocks/")
            print(f"   ğŸ“¦ Toplam: {total_size_mb:.0f} MB ({total_size_mb/1024:.1f} GB)")
        except ImportError:
            print("â„¹ï¸  Colab deÄŸil, Drive kopyalama atlandÄ±.")
        except Exception as e:
            print(f"âš ï¸  Drive kopyalama hatasÄ±: {e}")
    
    # Router kalibrasyonu (opsiyonel â€” T4'te cihaz sorunu olabilir)
    try:
        print(f"\nğŸ“ Router kalibrasyonu baÅŸlÄ±yor...")
        loader.calibrate_router(num_steps=200)
        
        block_indices2, weights2 = loader.predict_blocks(prompt, prefetch=False)
        print(f"ğŸ”® Kalibre edilmiÅŸ blok tahmini:")
        print(f"   Tahmin edilen bloklar: {block_indices2}")
        print(f"   AÄŸÄ±rlÄ±klar: {[f'{w:.2%}' for w in weights2.tolist()]}")
    except Exception as e:
        print(f"âš ï¸  Kalibrasyon hatasÄ± (bloklar yine de kaydedildi): {e}")
    
    # Metin Ã¼retimi
    try:
        print(f"\nğŸ”„ Metin Ã¼retimi baÅŸlÄ±yor (sharding modu)...")
        generated = loader.generate(
            prompt=prompt,
            max_new_tokens=100,
            temperature=0.8,
            top_k=40,
        )
        print(f"\nğŸ“ Ãœretilen metin:")
        print(f"'{generated}'")
    except Exception as e:
        print(f"âš ï¸  Metin Ã¼retimi hatasÄ±: {e}")
    
    return loader


def run_lazy_loading_demo(tokenizer, save_dir: str, prompt: str):
    """Lazy loading demo: Diskten yÃ¼kleme + sequential all blok Ã§alÄ±ÅŸtÄ±rma."""
    from swarm_llm.hf_loader import HuggingFaceBlockLoader
    
    print("\n" + "="*70)
    print("ğŸ“‚ LAZY LOADING MODU (Sequential All)")
    print("="*70)
    print("   TÃ¼m bloklar sÄ±rayla Ã§alÄ±ÅŸÄ±r â€” NO-SHARDING kalitesi + VRAM tasarrufu")
    print("   Her blok: diskten yÃ¼kle â†’ Ã§alÄ±ÅŸtÄ±r â†’ bellekten sil\n")
    
    if not os.path.exists(save_dir):
        print(f"âš ï¸  {save_dir} bulunamadÄ±! Ã–nce sharding modunu Ã§alÄ±ÅŸtÄ±rÄ±n.")
        return None
    
    loader_lazy = HuggingFaceBlockLoader.from_disk_blocks(
        tokenizer=tokenizer,
        save_dir=save_dir,
        lazy_load=True,
        device="auto",
        sequential_all=True,  # TÃ¼m bloklar sÄ±rayla Ã§alÄ±ÅŸÄ±r (kaliteli)
    )
    
    # Prefetching baÅŸlat
    loader_lazy.start_prefetching()
    
    print(f"âœ… Lazy loader hazÄ±r (prefetching aktif)")
    print(f"   Bloklar diskte: {loader_lazy.num_blocks} blok")
    
    # Metin Ã¼retimi
    print(f"\nğŸ”„ Metin Ã¼retimi (lazy loading + sequential all)...")
    generated = loader_lazy.generate(
        prompt=prompt,
        max_new_tokens=50,
        temperature=0.8,
        top_k=40,
        prefetch_next=False,  # Sequential all'da prefetch'e gerek yok
    )
    
    print(f"\nğŸ“ Ãœretilen metin:")
    print(f"'{generated}'")
    
    # Prefetching durdur
    loader_lazy.stop_prefetching()
    
    return loader_lazy


def main():
    parser = argparse.ArgumentParser(
        description="Parisi-Nash Router: HuggingFace Model Dinamik Blok YÃ¼kleme Demo"
    )
    parser.add_argument("--model", type=str, default="Qwen/Qwen2.5-7B",
                       help="HuggingFace model adÄ± (default: Qwen/Qwen2.5-7B)")
    parser.add_argument("--num-blocks", type=int, default=7,
                       help="Blok sayÄ±sÄ± (default: 7)")
    parser.add_argument("--top-k", type=int, default=2,
                       help="Her forward'da Ã§alÄ±ÅŸacak blok sayÄ±sÄ± (default: 2)")
    parser.add_argument("--prompt", type=str, default="The history of artificial intelligence is",
                       help="Test prompt'u")
    parser.add_argument("--save-dir", type=str, default="model_blocks_qwen25_7b",
                       help="BloklarÄ±n kaydedileceÄŸi dizin")
    parser.add_argument("--drive-dir", type=str, default="swarm_model_blocks",
                       help="Google Drive'daki hedef klasÃ¶r adÄ±")
    parser.add_argument("--test-only", action="store_true",
                       help="Sadece NO-SHARDING testi Ã§alÄ±ÅŸtÄ±r")
    parser.add_argument("--skip-lazy", action="store_true",
                       help="Lazy loading testini atla")
    args = parser.parse_args()
    
    # GPU bilgisi
    print(f"CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    # Token al
    hf_token = get_hf_token()
    if not hf_token:
        print("âŒ HF Token bulunamadÄ±. Ã‡Ä±kÄ±lÄ±yor.")
        sys.exit(1)
    
    # Model yÃ¼kle
    model, tokenizer = load_model(args.model, hf_token)
    
    # 1. NO-SHARDING testi
    no_sharding_ok = test_no_sharding(model, tokenizer, args.prompt)
    
    if args.test_only:
        print("\nâœ… Test tamamlandÄ±.")
        return
    
    if not no_sharding_ok:
        print("\nâš ï¸  NO-SHARDING testi baÅŸarÄ±sÄ±z. Sharding modunu atlamak isteyebilirsiniz.")
        response = input("Devam etmek istiyor musunuz? (e/h): ").strip().lower()
        if response != 'e':
            return
    
    # 2. Sharding modu
    loader = run_sharded_demo(model, tokenizer, args.num_blocks, args.top_k, args.prompt, args.save_dir)
    
    # 3. Model'i RAM'den kaldÄ±r
    del model
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    print(f"\nâœ… Model RAM'den kaldÄ±rÄ±ldÄ±")
    
    # 4. Lazy loading
    if not args.skip_lazy:
        run_lazy_loading_demo(tokenizer, args.save_dir, args.prompt)
    
    # SonuÃ§
    print("\n" + "="*70)
    print("ğŸ“Š SONUÃ‡LAR")
    print("="*70)
    print("âœ… SÄ±fÄ±r eÄŸitim maliyeti: Mevcut modeli yeniden eÄŸitmeden kullandÄ±k")
    print("âœ… Dinamik RAM yÃ¶netimi: Sadece gerekli bloklar RAM'de")
    print("âœ… Sticky Routing: Thrashing Ã¶nlendi")
    print("âœ… Lazy Loading: Bloklar diskten gerektiÄŸinde yÃ¼klendi")
    print("âœ… Vocab Alignment: Otomatik kontrol yapÄ±ldÄ±")
    print("âœ… Accelerate hook temizleme: T4 uyumluluÄŸu")


if __name__ == "__main__":
    main()
