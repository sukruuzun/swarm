# Qwen2.5-7B ile HÄ±zlÄ± BaÅŸlangÄ±Ã§

## 1. Model YÃ¼kleme (Accelerate ile)

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from swarm_llm.hf_loader import HuggingFaceBlockLoader
import torch

# HuggingFace token (Colab Secrets'tan veya environment variable'dan)
import os
hf_token = os.environ.get('HF_TOKEN')  # Token'Ä±nÄ± buraya ayarla veya Colab Secrets kullan

# Qwen2.5-7B yÃ¼kle
model_name = "Qwen/Qwen2.5-7B"

tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    token=hf_token,
    torch_dtype=torch.float16,  # VRAM tasarrufu
    device_map="auto",          # Accelerate otomatik daÄŸÄ±tÄ±m
)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
```

## 2. Bloklara BÃ¶lme

Qwen2.5-7B: **28 layer** â†’ 7 blok x 4 layer (veya 8 blok x 3.5 layer â†’ 8 blok)

```python
# Qwen2.5-7B iÃ§in Ã¶nerilen yapÄ±landÄ±rma
loader = HuggingFaceBlockLoader(
    model=model,
    tokenizer=tokenizer,
    num_blocks=7,      # 28 layer â†’ 7 blok x 4 layer
    top_k=2,           # Her forward'da sadece 2 blok
    device="auto",
)

print(f"âœ… Qwen2.5-7B bloklara bÃ¶lÃ¼ndÃ¼")
print(f"   Toplam layer: {len(loader.layers)}")
print(f"   Blok sayÄ±sÄ±: {loader.num_blocks}")
print(f"   Her blok: {loader.layers_per_block} layer")
```

## 3. Ã–nceden Tahmin

```python
prompt = "The history of artificial intelligence"
block_indices, weights = loader.predict_blocks(prompt, prefetch=True)

print(f"ğŸ”® Tahmin: Bloklar {block_indices}")
print(f"   AÄŸÄ±rlÄ±klar: {[f'{w:.2%}' for w in weights.tolist()]}")
```

## 4. Forward Pass

```python
input_ids = tokenizer.encode(prompt, return_tensors="pt")
# Accelerate ile daÄŸÄ±tÄ±lmÄ±ÅŸ modellerde device otomatik handle edilir
outputs = loader.forward(input_ids)

print(f"âœ… Ã‡alÄ±ÅŸtÄ±rÄ±lan bloklar: {outputs['selected_indices']}")
```

## 5. Metin Ãœretimi (Asenkron Prefetching ile)

```python
# Prefetching'i baÅŸlat
loader.start_prefetching()

generated = loader.generate(
    prompt="The future of AI is",
    max_new_tokens=100,
    temperature=0.8,
    top_k=40,
    prefetch_next=True,  # Bir sonraki adÄ±mÄ±n bloklarÄ±nÄ± Ã¶nceden yÃ¼kle
)

print(generated)
```

## 6. Lazy Loading (Diskten YÃ¼kleme)

```python
# Modeli diske kaydet
save_dir = "model_blocks_qwen25_7b"
loader.save_blocks_to_disk(save_dir)

# Modeli RAM'den kaldÄ±r
del model
torch.cuda.empty_cache()

# Diskten lazy yÃ¼kle
loader_lazy = HuggingFaceBlockLoader.from_disk_blocks(
    tokenizer=tokenizer,
    save_dir=save_dir,
    lazy_load=True,
)

# Prefetching'i baÅŸlat
loader_lazy.start_prefetching()

# Forward: Sadece seÃ§ilen bloklar diskten yÃ¼klenir
outputs = loader_lazy.forward(input_ids)
```

## Qwen2.5-7B Ã–zellikleri

- **Parametre:** ~7B
- **Layer sayÄ±sÄ±:** 28
- **Ã–nerilen blok yapÄ±sÄ±:** 7 blok x 4 layer
- **VRAM (tÃ¼m model):** ~14GB (float16)
- **VRAM (lazy, top_k=2):** ~4GB
- **Tasarruf:** ~3.5x

## Sorun Giderme

### Accelerate UyarÄ±sÄ±
```
WARNING:accelerate.big_modeling:You shouldn't move a model that is dispatched...
```
âœ… **Normal:** `device_map="auto"` kullanÄ±ldÄ±ÄŸÄ±nda bu uyarÄ± gÃ¶rÃ¼lebilir. Kod otomatik handle eder.

### Model YapÄ±sÄ±
Qwen modelleri genelde `model.model.layers` yapÄ±sÄ±nda. Kod otomatik bulur:
- `model.model.layers` âœ…
- `model.layers` âœ…
- `model.transformer.h` âœ…

### VRAM Yetersizse
```python
# Daha fazla blok, daha az top_k
loader = HuggingFaceBlockLoader(
    model=model,
    tokenizer=tokenizer,
    num_blocks=14,     # 28 layer â†’ 14 blok x 2 layer
    top_k=1,           # Her forward'da sadece 1 blok
)
```
