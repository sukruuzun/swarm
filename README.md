# Swarm-LLM: Parisi-Nash Teoremi ile Dinamik Model YÃ¶netimi

Parisi'nin sÄ±ÄŸÄ±rcÄ±k sÃ¼rÃ¼sÃ¼ modeli ve Nash oyun teorisini birleÅŸtirerek, eÄŸitilmiÅŸ dev modelleri (Llama, Qwen) **sÄ±fÄ±r eÄŸitim maliyetiyle** dinamik olarak yÃ¶neten bir sistem.

## ğŸ¯ Temel Ã–zellikler

### 1. **Ã–nceden Tahmin MekanizmasÄ±** (`get_predictive_indices`)
Modeli Ã§alÄ±ÅŸtÄ±rmadan, sadece giriÅŸ cÃ¼mlesine bakarak hangi bloklarÄ±n gerekli olduÄŸunu tahmin eder.

```python
from swarm_llm.hf_loader import HuggingFaceBlockLoader

loader = HuggingFaceBlockLoader(model, tokenizer, num_blocks=8, top_k=2)

# Tahmin: hangi bloklar gerekli?
block_indices, weights = loader.predict_blocks("The history of science")
print(f"YÃ¼klenecek bloklar: {block_indices}")  # [2, 5]
```

### 2. **SÄ±fÄ±r EÄŸitim Maliyeti**
Mevcut eÄŸitilmiÅŸ bir modelin (Llama gibi) katmanlarÄ±nÄ± bloklara yerleÅŸtirdiÄŸimizde, modeli yeniden eÄŸitmeden teoreminle "yÃ¶netmeye" baÅŸlÄ±yoruz.

### 3. **Dinamik RAM YÃ¶netimi**
EÄŸer 10 bloktan sadece 2'sini yÃ¼klersen, 16 GB VRAM isteyen bir modeli ~3.2 GB VRAM ile Ã§alÄ±ÅŸtÄ±rabilirsin.

**Ã–rnek:** Llama-2-7B (32 layer) â†’ 8 blok x 4 layer â†’ Her forward'da sadece 2 blok â†’ **~4x VRAM tasarrufu**

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### Colab'da Ã‡alÄ±ÅŸtÄ±r

```bash
# GitHub'dan klonla
git clone https://github.com/YOUR_USERNAME/swarm.git
cd swarm

# Colab notebook'u aÃ§
# colab_demo_hf.ipynb
```

### Yerel Kurulum

```bash
pip install -r requirements.txt
```

### KullanÄ±m Ã–rneÄŸi

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from swarm_llm.hf_loader import HuggingFaceBlockLoader

# Model yÃ¼kle
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

# Bloklara bÃ¶l ve router ekle
loader = HuggingFaceBlockLoader(
    model=model,
    tokenizer=tokenizer,
    num_blocks=4,      # 12 layer â†’ 4 blok x 3 layer
    top_k=2,           # Her forward'da sadece 2 blok
)

# Ã–nceden tahmin
prompt = "The history of science is"
block_indices, weights = loader.predict_blocks(prompt)
print(f"YÃ¼klenecek bloklar: {block_indices}")

# Forward: sadece seÃ§ilen bloklar Ã§alÄ±ÅŸÄ±r
outputs = loader.generate(prompt, max_new_tokens=50)
print(outputs)
```

## ğŸ“Š VRAM Tasarrufu

```python
savings = loader.estimate_vram_savings()
print(f"Tasarruf oranÄ±: {savings['savings_ratio']:.1f}x")
# Ã–rnek: 70B model â†’ 17.5B aktif (top_k=2, num_blocks=8)
```

## ğŸ—ï¸ Mimari

### External Router (DÄ±ÅŸsal YÃ¶nlendirici)
- **Parisi-Nash Router**: MLP gate + temperature annealing + load balancing
- **Ã–nceden tahmin**: `get_predictive_indices()` - modeli Ã§alÄ±ÅŸtÄ±rmadan blok seÃ§imi
- **Dinamik seÃ§im**: Her forward'da router hangi bloklarÄ±n Ã§alÄ±ÅŸacaÄŸÄ±na karar verir

### Sparse Block Loader
- **Blok yÃ¶netimi**: Modeli N bloÄŸa bÃ¶ler (Ã¶rn. 8 blok)
- **Seyrek aktivasyon**: Her forward'da sadece top_k blok Ã§alÄ±ÅŸÄ±r (Ã¶rn. 2 blok)
- **AÄŸÄ±rlÄ±klÄ± birleÅŸtirme**: Router'Ä±n verdiÄŸi aÄŸÄ±rlÄ±klarla blok Ã§Ä±ktÄ±larÄ±nÄ± birleÅŸtirir

## ğŸ“ Proje YapÄ±sÄ±

```
swarm/
â”œâ”€â”€ swarm_llm/
â”‚   â”œâ”€â”€ external_router.py      # Parisi-Nash router (blok seÃ§ici)
â”‚   â”œâ”€â”€ sparse_loader.py        # Sparse block loader (genel)
â”‚   â”œâ”€â”€ hf_loader.py            # HuggingFace entegrasyonu
â”‚   â”œâ”€â”€ unified.py              # SÄ±fÄ±rdan eÄŸitim iÃ§in unified model
â”‚   â””â”€â”€ ...
â”œâ”€â”€ colab_demo_hf.ipynb         # Colab demo (HF entegrasyonu)
â”œâ”€â”€ demo_external_router.py     # Yerel demo
â””â”€â”€ requirements.txt
```

## ğŸ”¬ Teoreminin AÃ§Ä±sÄ±ndan Ã–nemi

### Tahmin MekanizmasÄ±
`get_predictive_indices` kÄ±smÄ± teoreminin beyni. Modelin tamamÄ±nÄ± Ã§alÄ±ÅŸtÄ±rmadan, sadece giriÅŸ cÃ¼mlesine bakarak "Benim 10 bloktan sadece 2. bloÄŸa ihtiyacÄ±m var" diyor.

### SÄ±fÄ±r EÄŸitim Maliyeti
Mevcut eÄŸitilmiÅŸ bir modelin (Llama gibi) katmanlarÄ±nÄ± bu bloklara yerleÅŸtirdiÄŸimizde, modeli yeniden eÄŸitmeden teoreminle "yÃ¶netmeye" baÅŸlÄ±yoruz.

### Dinamik RAM YÃ¶netimi
EÄŸer 10 bloktan sadece 2'sini yÃ¼klersen, 16 GB VRAM isteyen bir modeli ~3.2 GB VRAM ile Ã§alÄ±ÅŸtÄ±rabilirsin.

## ğŸ“ KullanÄ±m SenaryolarÄ±

1. **Evdeki laptopta Llama 70B Ã§alÄ±ÅŸtÄ±rmak**
   - 70B â†’ 8 blok â†’ top_k=2 â†’ ~17.5B aktif
   - 140GB VRAM â†’ ~35GB VRAM

2. **Colab'da bÃ¼yÃ¼k modeller**
   - T4 (16GB) â†’ Llama-2-7B â†’ 4 blok â†’ top_k=1 â†’ ~1.75B aktif

3. **Edge cihazlar**
   - KÃ¼Ã§Ã¼k VRAM â†’ Sadece gerekli bloklarÄ± yÃ¼kle

## ğŸ“ Notlar

- **Desteklenen modeller**: Llama, Qwen, Mistral, GPT-2 (ve benzeri transformer mimarileri)
- **Router fine-tuning**: Router'Ä± ince ayar yaparak blok seÃ§imini iyileÅŸtirebilirsin
- **Python 3.9+**: TÃ¼m kod Python 3.9 uyumlu

## ğŸ¤ KatkÄ±da Bulunma

Pull request'ler memnuniyetle karÅŸÄ±lanÄ±r! BÃ¼yÃ¼k deÄŸiÅŸiklikler iÃ§in Ã¶nce bir issue aÃ§arak neyi deÄŸiÅŸtirmek istediÄŸini tartÄ±ÅŸalÄ±m.

## ğŸ“„ Lisans

MIT License

## ğŸ™ TeÅŸekkÃ¼rler

- Parisi'nin sÄ±ÄŸÄ±rcÄ±k sÃ¼rÃ¼sÃ¼ modeli
- Nash oyun teorisi
- HuggingFace transformers
