{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parisi-Nash External Router: Qwen2.5-7B ile Dinamik Blok YÃ¼kleme\n",
    "\n",
    "Bu notebook, Qwen2.5-7B modelini Parisi-Nash teoremi ile \"dilimleyip\" yÃ¶netmeyi gÃ¶sterir.\n",
    "\n",
    "**Ã–zellikler:**\n",
    "- âœ… SÄ±fÄ±r eÄŸitim maliyeti: Mevcut modeli yeniden eÄŸitmeden kullanÄ±r\n",
    "- âœ… Dinamik RAM yÃ¶netimi: 7B parametre â†’ sadece 2B RAM'de\n",
    "- âœ… Sticky Routing: Bloklar belirli bir sÃ¼re sabit kalÄ±r (thrashing Ã¶nleme)\n",
    "- âœ… Lazy Loading: Bloklar diskten gerektiÄŸinde yÃ¼klenir\n",
    "- âœ… Vocab Alignment: Tokenizer ve model vocab_size kontrolÃ¼\n",
    "- âœ… GÃ¼venli Token YÃ¶netimi: HF Token aÃ§Ä±k metin olarak saklanmaz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kurulum\n",
    "!pip install -q torch transformers accelerate\n",
    "\n",
    "# HuggingFace token (GÃ¼venli yÃ¶ntemler)\n",
    "# YÃ¶ntem 1: Colab Secrets (Ã¶nerilen) â†’ Sol panelde ğŸ”‘ simgesine tÄ±kla, HF_TOKEN ekle\n",
    "# YÃ¶ntem 2: Environment variable â†’ export HF_TOKEN=hf_xxx...\n",
    "import os\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
    "    print('âœ… Token Colab Secrets\\'tan alÄ±ndÄ±')\n",
    "except:\n",
    "    if 'HF_TOKEN' not in os.environ:\n",
    "        os.environ['HF_TOKEN'] = input('ğŸ”‘ HF Token giriniz: ')\n",
    "\n",
    "# GitHub repo'yu klonla ve gÃ¼ncelle\n",
    "!git clone https://github.com/sukruuzun/swarm.git 2>/dev/null || (cd swarm && git pull origin main)\n",
    "import sys\n",
    "sys.path.insert(0, 'swarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from swarm_llm.hf_loader import HuggingFaceBlockLoader\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Qwen2.5-7B Model YÃ¼kleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen2.5-7B yÃ¼kle\n",
    "import os\n",
    "hf_token = os.environ.get('HF_TOKEN')\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B\"\n",
    "\n",
    "print(f\"ğŸš€ Qwen2.5-7B yÃ¼kleniyor: {model_name}\")\n",
    "print(\"   âš ï¸  Bu iÅŸlem birkaÃ§ dakika sÃ¼rebilir...\")\n",
    "\n",
    "tokenizer_qwen = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
    "model_qwen = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token=hf_token,\n",
    "    torch_dtype=torch.float16,  # VRAM tasarrufu iÃ§in\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "if tokenizer_qwen.pad_token is None:\n",
    "    tokenizer_qwen.pad_token = tokenizer_qwen.eos_token\n",
    "\n",
    "print(f\"âœ… Model yÃ¼klendi: {sum(p.numel() for p in model_qwen.parameters()) / 1e9:.1f}B parametre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NO-SHARDING Test Modu (Sorun TanÄ±lama)\n",
    "\n",
    "**EÄŸer karakter bozulmasÄ± veya \"The\" sonrasÄ± saÃ§malama varsa, Ã¶nce bu testi Ã§alÄ±ÅŸtÄ±r:**\n",
    "\n",
    "- `no_sharding=True`: Modeli hiÃ§ bÃ¶lmeden tek blok olarak Ã§alÄ±ÅŸtÄ±rÄ±r\n",
    "- **Ã‡alÄ±ÅŸÄ±yorsa**: Sorun sharding'de (bloklara bÃ¶lme iÅŸleminde)\n",
    "- **Hala bozuksa**: Sorun model yÃ¼kleme veya tokenizer'da\n",
    "\n",
    "Bu test, sorunun kaynaÄŸÄ±nÄ± belirlemek iÃ§in kritik!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO-SHARDING TEST: Modeli hiÃ§ bÃ¶lmeden tek blok olarak Ã§alÄ±ÅŸtÄ±r\n",
    "\n",
    "print(\"ğŸ”¬ NO-SHARDING TEST MODU: Model hiÃ§ bÃ¶lÃ¼nmeden Ã§alÄ±ÅŸacak\")\n",
    "print(\"   EÄŸer bu modda Ã§alÄ±ÅŸÄ±yorsa: Sorun sharding'de\")\n",
    "print(\"   EÄŸer hala bozuksa: Sorun model yÃ¼kleme veya tokenizer'da\\n\")\n",
    "\n",
    "loader_qwen_no_sharding = HuggingFaceBlockLoader(\n",
    "    model=model_qwen,\n",
    "    tokenizer=tokenizer_qwen,\n",
    "    no_sharding=True,  # ğŸ”‘ Modeli hiÃ§ bÃ¶lme, tÃ¼m katmanlarÄ± tek blokta Ã§alÄ±ÅŸtÄ±r\n",
    "    device=\"auto\",\n",
    ")\n",
    "\n",
    "print(f\"âœ… NO-SHARDING modu aktif\")\n",
    "print(f\"   Toplam layer: {len(loader_qwen_no_sharding.layers)}\")\n",
    "print(f\"   TÃ¼m katmanlar tek blokta: Block 0\")\n",
    "print(f\"   Router bypass edildi (tek blok olduÄŸu iÃ§in)\\n\")\n",
    "\n",
    "# Test: Metin Ã¼retimi\n",
    "prompt_test = \"The history of artificial intelligence is\"\n",
    "\n",
    "print(f\"ğŸ§ª Test Prompt: '{prompt_test}'\")\n",
    "print(f\"ğŸ”„ Metin Ã¼retimi baÅŸlÄ±yor (NO-SHARDING modu)...\\n\")\n",
    "\n",
    "generated_test = loader_qwen_no_sharding.generate(\n",
    "    prompt=prompt_test,\n",
    "    max_new_tokens=50,  # KÄ±sa test iÃ§in\n",
    "    temperature=0.8,\n",
    "    top_k=40,\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“ Ãœretilen metin (NO-SHARDING):\")\n",
    "print(f\"'{generated_test}'\\n\")\n",
    "\n",
    "# SonuÃ§ analizi\n",
    "if \"ç„˜\" in generated_test or \"æº¦\" in generated_test or len(generated_test.split()) < 5:\n",
    "    print(\"âŒ SONUÃ‡: NO-SHARDING modunda da sorun var!\")\n",
    "    print(\"   â†’ Sorun sharding'de DEÄÄ°L, model yÃ¼kleme veya tokenizer'da\")\n",
    "    print(\"   â†’ Vocab size kontrolÃ¼ yapÄ±ldÄ± mÄ±? YukarÄ±daki loglarÄ± kontrol et\")\n",
    "else:\n",
    "    print(\"âœ… SONUÃ‡: NO-SHARDING modunda Ã§alÄ±ÅŸÄ±yor!\")\n",
    "    print(\"   â†’ Sorun sharding'de (bloklara bÃ¶lme iÅŸleminde)\")\n",
    "    print(\"   â†’ Normal sharding modunu kullanma, sorun devam edecek\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Qwen2.5-7B Bloklara BÃ¶lme (Normal Mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen'i bloklara bÃ¶l (28 layer â†’ 7 blok x 4 layer)\n",
    "loader_qwen = HuggingFaceBlockLoader(\n",
    "    model=model_qwen,\n",
    "    tokenizer=tokenizer_qwen,\n",
    "    num_blocks=7,      # 28 layer â†’ 7 blok x 4 layer\n",
    "    top_k=2,           # Her forward'da sadece 2 blok\n",
    "    device=\"auto\",\n",
    ")\n",
    "\n",
    "print(f\"âœ… Qwen2.5-7B bloklara bÃ¶lÃ¼ndÃ¼\")\n",
    "print(f\"   Toplam layer: {len(loader_qwen.layers)}\")\n",
    "print(f\"   Blok sayÄ±sÄ±: {loader_qwen.num_blocks}\")\n",
    "print(f\"   Her blok: {loader_qwen.layers_per_block} layer\")\n",
    "print(f\"   Her forward'da: {loader_qwen.top_k}/{loader_qwen.num_blocks} blok Ã§alÄ±ÅŸÄ±r\")\n",
    "\n",
    "# VRAM tasarrufu analizi\n",
    "savings_qwen = loader_qwen.estimate_vram_savings()\n",
    "print(f\"\\nğŸ’¾ VRAM Tasarrufu:\")\n",
    "print(f\"   TÃ¼m model: {savings_qwen['total_vram_gb']:.1f} GB\")\n",
    "print(f\"   Seyrek ({savings_qwen['blocks_loaded']} blok): {savings_qwen['sparse_vram_gb']:.1f} GB\")\n",
    "print(f\"   Tasarruf: {savings_qwen['savings_ratio']:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BloklarÄ± Diske Kaydetme (Lazy Loading iÃ§in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen'i diske kaydet (lazy loading iÃ§in)\n",
    "save_dir_qwen = \"model_blocks_qwen25_7b\"\n",
    "\n",
    "print(\"ğŸ’¾ Qwen2.5-7B bloklarÄ± diske kaydediliyor...\")\n",
    "loader_qwen.save_blocks_to_disk(save_dir_qwen)\n",
    "\n",
    "# Modeli RAM'den kaldÄ±r\n",
    "del model_qwen\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nâœ… Model RAM'den kaldÄ±rÄ±ldÄ±\")\n",
    "print(f\"   Åu an RAM'de: Sadece router + embedding\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   VRAM: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Diskten Lazy Loading (Asenkron Prefetching ile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diskten lazy loading ile yÃ¼kle (asenkron prefetching ile)\n",
    "loader_qwen_lazy = HuggingFaceBlockLoader.from_disk_blocks(\n",
    "    tokenizer=tokenizer_qwen,\n",
    "    save_dir=save_dir_qwen,\n",
    "    lazy_load=True,\n",
    "    device=\"auto\",\n",
    ")\n",
    "\n",
    "# Asenkron prefetching'i baÅŸlat\n",
    "loader_qwen_lazy.start_prefetching()\n",
    "\n",
    "print(\"âœ… Lazy loader hazÄ±r (asenkron prefetching aktif)\")\n",
    "print(f\"   Bloklar diskte: {loader_qwen_lazy.num_blocks} blok\")\n",
    "print(f\"   Router tahmin ettiÄŸi bloklar arka planda yÃ¼klenecek\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Metin Ãœretimi (Sticky Routing ile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen ile metin Ã¼retimi (sticky routing + asenkron prefetching)\n",
    "prompt_qwen = \"The history of artificial intelligence is\"\n",
    "\n",
    "print(f\"GiriÅŸ: '{prompt_qwen}'\")\n",
    "print(f\"\\nğŸ”„ Metin Ã¼retimi baÅŸlÄ±yor (sticky routing + prefetching aktif)...\")\n",
    "\n",
    "generated_qwen = loader_qwen_lazy.generate(\n",
    "    prompt=prompt_qwen,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.8,\n",
    "    top_k=40,\n",
    "    prefetch_next=True,  # ğŸ”‘ Bir sonraki adÄ±mÄ±n bloklarÄ±nÄ± Ã¶nceden yÃ¼kle\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“ Ãœretilen metin:\")\n",
    "print(f\"'{generated_qwen}'\")\n",
    "print(f\"\\nğŸ’¡ Ã–zellikler:\")\n",
    "print(f\"   1. Sticky Routing: Bloklar 25 token boyunca sabit kalÄ±r (thrashing Ã¶nleme)\")\n",
    "print(f\"   2. Asenkron Prefetching: Router'Ä±n tahmin ettiÄŸi bloklar arka planda yÃ¼klenir\")\n",
    "print(f\"   3. Block Locking: Block 0 ve 1 RAM'de kilitli (sÃ¼rekli kullanÄ±lan bloklar)\")\n",
    "print(f\"   4. Vocab Alignment: Tokenizer ve model vocab_size kontrolÃ¼ otomatik\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. FarklÄ± Promptlar ile Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FarklÄ± promptlar ile test\n",
    "test_prompts = [\n",
    "    \"Quantum computing will change\",\n",
    "    \"The best way to learn programming is\",\n",
    "    \"In the year 2050, humanity will\",\n",
    "    \"The relationship between mathematics and physics\",\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt {i+1}: '{prompt}'\")\n",
    "    \n",
    "    # Hangi bloklarÄ± seÃ§tiÄŸini gÃ¶ster\n",
    "    block_indices, weights = loader_qwen.predict_blocks(prompt)\n",
    "    print(f\"ğŸ”® Tahmin: Bloklar {block_indices} (aÄŸÄ±rlÄ±klar: {[f'{w:.1%}' for w in weights.tolist()]})\")\n",
    "    \n",
    "    generated = loader_qwen.generate(\n",
    "        prompt=prompt,\n",
    "        max_new_tokens=60,\n",
    "        temperature=0.8,\n",
    "        top_k=40,\n",
    "    )\n",
    "    print(f\"ğŸ“ SonuÃ§: '{generated}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SonuÃ§\n",
    "\n",
    "âœ… **Teoreminin baÅŸarÄ±sÄ±:** Modeli Ã§alÄ±ÅŸtÄ±rmadan, sadece giriÅŸ embedding'ine bakarak hangi bloklarÄ±n gerekli olduÄŸunu tahmin edebiliyoruz.\n",
    "\n",
    "âœ… **SÄ±fÄ±r eÄŸitim maliyeti:** Mevcut eÄŸitilmiÅŸ modeli yeniden eÄŸitmeden, sadece router ile yÃ¶netiyoruz.\n",
    "\n",
    "âœ… **Dinamik RAM yÃ¶netimi:** 7B parametre â†’ sadece 2B RAM'de Ã§alÄ±ÅŸtÄ±rabiliriz.\n",
    "\n",
    "âœ… **Lazy SSD Loader:** Modelin tamamÄ± diskte, sadece gereken bloklar RAM'e yÃ¼klenir.\n",
    "   - 14GB VRAM â†’ ~4GB VRAM (7B model, top_k=2)\n",
    "   - GerÃ§ek zamanlÄ± diskten yÃ¼kleme\n",
    "   - KullanÄ±lmayan bloklar otomatik RAM'den kaldÄ±rÄ±lÄ±r\n",
    "\n",
    "âœ… **Sticky Routing:** Router bir kez blok seÃ§er, sonraki 25 token boyunca aynÄ± bloklar kullanÄ±lÄ±r.\n",
    "   - Thrashing Ã¶nlenir (sÃ¼rekli yÃ¼kleme/silme dÃ¶ngÃ¼sÃ¼ yok)\n",
    "   - SSD I/O minimize edilir\n",
    "   - Performans artar\n",
    "\n",
    "âœ… **Vocab Alignment:** Tokenizer ve model vocab_size'larÄ± otomatik kontrol edilir.\n",
    "   - Karakter kaymasÄ± (offset) hatalarÄ± tespit edilir\n",
    "   - Token ID mapping kontrolÃ¼ yapÄ±lÄ±r\n",
    "\n",
    "**Sonraki adÄ±mlar:**\n",
    "- Router'Ä± fine-tune ederek blok seÃ§imini iyileÅŸtirebilirsin\n",
    "- FarklÄ± `num_blocks` ve `top_k` kombinasyonlarÄ±nÄ± deneyebilirsin\n",
    "- Lazy loading + sticky routing ile bÃ¼yÃ¼k modelleri dÃ¼ÅŸÃ¼k VRAM'li PC'lerde Ã§alÄ±ÅŸtÄ±rabilirsin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
